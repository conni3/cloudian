
[{"content":" Preface # The preface starts with a motivation to read this book - professionals of every computing specialty should understand both hardware and software. This point was emphasized when the authors mentioned the switch from uniprocessor to multicore microprocessors (which will be emphasized many more times throughout Chapter 1). Until tools (and researchers) can fully hide the parallel nature of today’s hardware, programmers must have at least a basic understanding of both hardware and software to write high-performance code.\nThe book uses RISC-V ISA, the instruction set the authors invented to explain computer architecture - which takes advantages of the elegance of MIPS intruction set while cleaning up its \u0026ldquo;quirks\u0026rdquo;.\nChapter 1 # The first chapter focuses on giving the readers basic understanding of the architecture, namely some keywords that will be used in the later sections. This was delivered with the context of past and current developments, design challenges, real-life examples while layering up the concepts in a seamless way. Another thing to add is that the book asks, or delivers information in a very thought provoking manner. There were many moments when I got lost in thought or were stuck in a dilemma to go on a \u0026ldquo;google/gpt spiral\u0026rdquo; or to continue reading the book.\nIntroduction # First section of the chapter introduction - gets the reader familiar with classes of computers, with emphasis on PMDs exceeding PC usage in the post-PC era and lightly mentions the constraints of computer architecture over the years. The constraints evolved from size of computer\u0026rsquo;s memory to parallel nature of processors and hierarchical nature of memories (which improved the performance of a C program by a factor of 200) and our most recent challenge - the power wall (which is explained in later sections).\n“Today’s science fiction suggests tomorrow’s killer applications: already on their way are glasses that augment reality, the cashless society, and cars that can drive themselves.”\nThis was a quote I found interesting and somewhat humorous. The book was written in 2019 and the \u0026ldquo;science fiction\u0026rdquo; then now has already become a reality.\nThe 8 great ideas in Computer Architecture # Moore\u0026rsquo;s law # According to Moore\u0026rsquo;s law, the number of transistors in IC (integrated circuit) will double every 2 years. This has been true over 50 years, although there has been some fluctuations. Therefore, a computer architect should always design their architecture based on how the industry will be when their work is done.\nAbstraction # Abstraction is another idea that helps hardware and software designers work more efficiently. The main idea is that each layer abstracts away the lower layer, essentially hides them idea. This is done at every level. Even in simple circuit analysis, components and parts of the circuit are replaced with a \u0026ldquo;box\u0026rdquo; with input and output.\nMaking the common case fast # It is often easier to make the common case faster than the rare case. And simpler to do so too.\nPerformance via Parallelism # Parallelism is heavily mentioned in the book (at least in the introduction and the first chapter). Idea is more people working on something, the faster it will be.\nOne noteworthy thing is the idea of parallelism came when scaling a single processor (by packing more transistors to it) reached its limits (due to constraints we will explore in here)\nPerformance via Pipelining # Pipelining - this can be understood by how a factory works. Every part does one specific thing and then passes it on to another part. At every stage, a single simple action is done. Simpler, repetitive means each step takes smaller time and has less error.\nPrediction # The quote I really liked was here was \u0026ldquo;It can be better to ask for forgiveness than to ask for permission, the next great idea is prediction\u0026rdquo;. One of the ideas that helped accelerate computing was prediction - assuming if you make a wrong prediction, you can recover from it with quickly.\nMemory Hierarchy # Finally, the memory hierarchy. People often think that accessing data on memory takes constant time. And that is the brilliance of what the earlier architects have accomplished. In reality, memory is composed of multiple types of memory - what we call \u0026ldquo;the memory hierarchy\u0026rdquo;\nIt looks like a pyramid. The one at the top is the smallest - faster, more expensive. The one at the bottom is the largest - slower, but cheaper. By the clever tricks employed by the designers, we, the users think that memory is as fast as the one on top, and as cheap as the one on the bottom.\nDependability by Redundancy # There are components in a computer that adds dependability (in case of a failure). This is a trade-off in performance, however, makes the system resilient to crashes or data loss.\nThis is the end of the first part, please read the other parts for the book or any article you find interesting :D\nReferences: Hennessy, J. L., \u0026amp; Patterson, D. A. (2018). Computer Organization and Design: RISC-V Edition (2nd ed.). Morgan Kaufmann/Elsevier.\n","externalUrl":null,"permalink":"/posts/computer-organization/patterson--hennessy-2020/chapter-1/part-1/","section":"Posts","summary":"Preface # The preface starts with a motivation to read this book - professionals of every computing specialty should understand both hardware and software.","title":"Part 1: Foundations \u0026 “Eight Great Ideas”","type":"posts"},{"content":" Below Your Program # For computers to run complex applications, there needs to be some sort of translation. Computers can only run simple operations; if we were to write the program for example for this website (although a simple application) - we need so much effort and time. This is the idea of abstraction.\nThere must be some way to convert these high level operations to a low level one. Usually, the bridge between hardware and application software (like this website) is called the systems software. Although, even for application softwares, we have different layers.\nA database system runs on top of systems software, and the application runs on top of the database systems.\nBut let\u0026rsquo;s get back to systems software. There are many types of systems software, but OS (operating system) and compiler are central to every system.\nAn OS (like Linux, iOS, and Windows) interfaces between user program and hardware. Some of its functions are:\nManaging input/output operations Allocating storage and memory Ensuring safe, concurrent use of the computer by multiple applications through protected sharing. Compilers translate high level languages like C++ or Java to instructions. We talk about this here but Chapter 2 explores this topic more.\nCompilers convert english like high level languages, say A + B to assembly language, add A, B. Assembler then convert this to binary or bits which is the alphabet that computers use. They also improve programmer productivity and be independent of the computer they were developed.\nUnder the Covers # Now this part is the bulk of this article. We take a look at the 5 classic components: input, output, memory, datapath and control. Datapath and control are sometimes combined to form the processor.\nThis is a figure from the book. The control sends signals that determine the operations of memory, datapath and input, output. This is what the book refers to as the \u0026ldquo;BIG PICTURE\u0026rdquo;. It is a nice way to visualize computer organization because all computers follow this structure.\nThe section here gives us an example: IPad 2. We see the parts laid out as shown above. Here, most of the components are IO.\nInput / Output: # Most PMDs use liquid crystal displays (LCDs). It is made of rod shaped molecules twisted in a helix and control whether to pass light through (not create them). Recent LCD technology has something called \u0026ldquo;the active matrix\u0026rdquo; - at every pixel, there is a transistor that controls the current. There are 3 transistors, each controlling the RGB (red, green, blue) color mask that determines the intensity of those colors.\nImage is represented as a matrix of bits, 8 bits for each color, and that matrix is called the bit map. The bit map is stored in raster refresh buffer, or frame buffer.\nNowadays, touch screens are replacing normal displays. One common way to implement a touch screen is to use capacitive sensing. Since, people are electrical conductors, transparent conductor is also placed over the glass. And touching distorts the field, changing capacitance.\nDatapath and Control: # The small L-shaped board next to the metal frame is the integrated circuits, more commonly called the chips. This contains the memory and the processor.\n\u0026ldquo;The processor logically comprises two main components: datapath and control, the respective brawn and brain of the processor\u0026rdquo; (Patterson and Hennessy, 2018, p.74)\nDatapath is responsible for the arithmetic operations and control manages what the other components (input, output, datapath and memory) do according to the instructions. This is explored more in Chapter 4.\nHere, instruction set architecture (ISA) is formally defined: a bridge between hardware and software where low level systems functions are handled and hidden away from application programmers. ISA along with OS is called the application binary interface (IBA).\nMemory is where the programs and data needed by the programs are stored while they are running. It is built from multiple DRAM chips. DRAM stands for dynamic RAM, where RAM means data is accessible in O(1) time as opposed to sequential access memories.\nIn the processor, there is another type of memory called SRAM. It is a temprorary memory used for caching - a memory where frequently used data is stored. Caches are always checked before accessing the main memory.\nBut SRAMs and DRAMs are volatile - meaning once the computer is powered off, all data is lost. The solution to that is nonvolatile memory technology. To help distinguish we call the memory while program is running (SRAMs and DRAMs) the main/primary memory and nonvolatile/permanent memory the secondary memory.\nAlthough magnetic disks dominated the secondary memory scene, nowadays we use flash memory. It is even replacing DRAMs in PMDs due to being much cheaper in addition to being nonvolatile. Memory is discussed more in Chapter 5.\nThe last part of this section discusses communication or computer networks and introduced more key concepts like LANs and WANs.\nTechnologies for Building Processors and Memory # This is where we discuss transistors - an electrical component that makes up all our computing devices. We discuss properties of transistors in a different article, but what we need to know right now is that it is a switch that has 2 states: on an off and is controlled by current. They are combined to make memory, logic gates, etc. (You can check out how transistors are used to make SRAM here.)\nThe technology used in computers first started with a vacuum tube in 1951, but after transistors were made in 1965, we saw an exponential increase. The evolution afterwards is integrated circuit, very large-scale integrated circuit (VLSI), and ultra large-scale integrated circuit (ULSI).\nHow are they actually made? # To make these circuits or chips, we start with a silicon wafer—a thin, flat slice of pure crystal. The idea is to draw your design onto the wafer using photolithography, which is kind of like shining light through a stencil to print patterns. The wafer is coated with light-sensitive material, and then a mask (the stencil) is used to shine UV light and “draw” the circuit. After this, we go through a cycle of adding material (like metal or insulators), removing parts we don’t need (etching), and modifying the silicon itself (called doping) to give it the right properties.\nThis process is repeated many times—layer after layer—to build up the transistors, wires, and all the components needed. Once it’s done, the wafer is cut up into small squares called dies, and each die is tested to see if it works. The working ones are packaged (usually in black rectangular casings with tiny pins or balls underneath) and shipped out for use in phones, laptops, etc. All of this is what people mean when they talk about \u0026ldquo;chip fabrication\u0026rdquo; or \u0026ldquo;semiconductor manufacturing\u0026rdquo;.\nFrom the author\nThank you for reading this, and special thanks to the authors of the book. Please check out other articles! 😇\nReferences: Hennessy, J. L., \u0026amp; Patterson, D. A. (2018). Computer Organization and Design: RISC-V Edition (2nd ed.). Morgan Kaufmann/Elsevier.\n","externalUrl":null,"permalink":"/posts/computer-organization/patterson--hennessy-2020/chapter-1/part-2/","section":"Posts","summary":"Below Your Program # For computers to run complex applications, there needs to be some sort of translation.","title":"Part 2: Inside the Machine – Abstraction Layers \u0026 Technologies","type":"posts"},{"content":" Performance # When you are choosing a computer, how should you choose? Is it the computer that performs operations the fastest, one that does the most work, or the one that consumes least power?\nIn data centers where they have a lot of jobs submitted by many users, they would care about the throughput or bandwidth - the total amount of work done in a given time. For example, which computer completed the most amount of jobs at the end of the day?\nFor us, individual users, we want to reduce the response time or the execution time - the time from start of a program to the end of a program.\n$$ \\text{Performance} = \\frac{1}{Execution \\space time} $$ From the math equation, we see that the smaller the execution time is, the higher the performance.\nNow, let\u0026rsquo;s look at how we can determine the execution time. But first, let\u0026rsquo;s understand what is the execution time. We call this wall clock time, response time, or elapse time. It measures the total time for a task: disk accesses + memory accesses + i/o activities + operating system overhead.\nBut it is common that computers are shared, and processors work on the multiple programs simultaneously. In that case, systems try to optimize throughput rather than time for one program. From that stems the distinction between elapsed time and CPU execution time or just CPU time.\nCPU time is further divided to user CPU time and system CPU time. We call the respective performances: CPU performance and system performance.\nIn many applications, they also rely on I/O performance. So we need to clearly define how to measure the performance and not get tangled up in all of the definitions and concepts. So let us declare something:\nTotal elapsed time will always be the one to determine our performance.\nWe should measure the total elapsed time first and then measure individual times to find where the bottleneck is.\nCPU execution time # There is a concept introduced here: clock cycle (or tick, clock tick, clock period, clock cycle). It is a discrete time that relates or measures how fast basic functions are executed.\nThe inverse is called clock rate and the relationship between the cycles for a program and cycle time can be summarized by the formulas below.\n$$ \\text{CPU execution time} = \\text{CPU clock cycles for a program} \\times \\text{Clock cycle time} $$ $$ \\text{CPU execution time} = \\frac{\\text{CPU clock cycles for a program}}{ \\text{Clock rate}} $$ We can see that to reduce execution time, we can either decrease the number of cycles for a program or increase the clock rate. However, we need to make sure decreasing the number of cycles don\u0026rsquo;t increase the clock rate or decreasing the clock rate doesn\u0026rsquo;t increase the number of cycles.\nThis is one example of why we should care about the elapsed time and not just measure performance in one dimension.\nWait, what even are clock cycles for a program? How do we measure that? Well, we know that processor executes instructions. Therefore, we can define cycles for a program to be the product of how many instructions the program needs and how long does it take to execute each instruction.\n$$ \\text{CPU clock cycles} = \\text{Instructions for a program} \\times \\text{Average clock cycles per instruction} $$ The average clock cycles per instruction is abbreviated as CPI. Since CPI is different for different instructions, we take the average of all instructions executed in the program. CPI gives us a way to compare different ISA for the same program.\nNow, we can combine the concepts and have the \u0026ldquo;classic CPU performance equation\u0026rdquo;: $$\r\\text{CPU time} = \\frac{\\text{Instruction count}\\times \\text{CPI}}{\\text{Clock rate}}\r$$ By default, we would know what the clock rate would be and we would know CPU time by measuring it. How do we know the instruction count? Well, the most accurate way to do that would be to actually measure it. And you can do that in software (by running a simulation) or directly in the hardware with the help of a counter.\nHardware can actually measure a variety of records, such as the number of instructions, average CPI, and even the sources of performance loss. This is the most reliable way because usually CPI depends on a lot of design choices: memory system and processor structure, as well as the program.\nHere is some components that affect the CPU performance:\nAlgorithm They determine the number of instructions as well as the type of instructions. For example, an algorithm that has more divides will have higher CPI. Programming language Statements in the language are translated to processor instructions (which determine the instruction count). Certain features also might affect - for example, Java has data abstraction that requires indirect calls, which results in higher CPI instructions Compiler Compiler converts source language instructions to machine instructions. So the compiler affects performance a lot. Instruction set architecture Affects all three aspect - instruction count, cpi and clock rate. Traditionally, clock rate is fixed but some processors like Intel i7 temporarily increases the clock rate until the chip gets too warm.\nThe Power Wall # Over the last 30 years, the clock rate and power increased rapidly but declined in the last few years due to one problem - the Power Wall. This comes from the inability to cool common microprocessors. Of course, there are ways to better cool the chips but it is an expensive solution and therefore - not scalable.\nIn the post-PC era, there is another problem with power. The devices are portable, which means the performance is limited by the battery.\nCMOS is the transistor that we use for integrated circuits. It\u0026rsquo;s main energy consumption is the energy that requires it to switch from 0 to 1 and vice versa - what we call dynamic energy. The dynamic energy depends on the capacitive load and voltage applied.\n$$ \\text{Dynamic Energy} \\space \\alpha \\space 1/2 \\times \\text{Capacitive load} \\times Voltage^2 $$ $$ \\text{Power} \\space \\alpha \\space 1/2 \\times \\text{Capacitive load} \\times Voltage^2 \\times \\text{Frequency switched}$$ Here, the capacitive load is a function of the fanout (number of transistors connected to output) and the technology. It determines the capacitance of both the wires and transistors.\nIn the last 30 years, clock rate grew by 1000 degrees while power consumption grew by only 30. That is because over the years, the voltage supplied decreased from 5V to 1V, and since power is proportional to voltage squared, we have such stark difference in increase.\nSo, why can\u0026rsquo;t we keep decreasing the voltage? The problem with that is it will make the transistors too leaky. Today, 40% of power consumption is due to leakage.\nNow that we have hit the power wall, people are now focusing on different ways to make the processor more efficient.\nThe Sea Change # Because we couldn\u0026rsquo;t improve due to Power Wall, the architects decided to ship microprocessors instead of trying to improve the performance of a single processor. The microprocessors contain multiple processors - called core in this case to avoid confusion. For example, quad microprocessors have 4 cores.\nPreviously, programs could increase in efficiency without a line of code being changed. But now with parallel processing, programmers need to be aware of the parallel nature of computers.\nOne way to use parallelism is pipelining, an instruction level parallelism. While parallelism was a very big stepping stone to get over power wall, there are so many reasons why it is a tool that must be handled carefully.\nIt is performance programming. Most of the systems we have (especially legacy ones like banking) are not built on parallel programming, and the effort to make them is very expensive (much more expensive than having more servers) that it is better for them to just write sequential programs.\nFor the program to make use of parallelism, it must divide the work equally, which might have some overhead that completely offsets the performance gained by parallelism. To take advantage of parallelism, we need:\nScheduling: Each core must have something to do at the same time. Load balancing: If one core takes too long, the others would be waiting and the benefits would be gone. So, we need to divide the work in terms of load (how long it will take), not the number of jobs. Reducing communication and synchronization overhead: If a process at one core depends on the outcome of others, it would increase the time taken. Or if there is too frequent communication, this will also take resources away from actual computing. There are some parallelism mentioned here, such as:\n1. Chapter 2: Parallelism and Instructions - Synchronization\nSynchronizing tasks (letting other cores know when they have completed their work)\n2. Chapter 3: Parallelism and Computer Arithmetic - Sub word Parallelism\nComputing elements in parallel (ex: multiplying two vectors)\n3. Chapter 4: Parallelism via Instructions\nPipelining and Prediction\nFallacies and Pitfalls # This section was one of my favorites. As the title suggest, the authors talked about fallacies - common misconceptions (conceptual or theoretical) and pitfalls - easily made mistakes (practical or implementation-related).\nOverall improvement of the computer will be proportional to the improvement made on one aspect\nHere, another concept was introduced - Amdahl\u0026rsquo;s Law. Amdahl’s Law says that the benefit of improving one part of a system—like making multiplication faster—is limited by how often that part is actually used.\nEven a big speedup won’t help much if it’s only used occasionally. This reflects the idea of diminishing returns: the more you optimize something that isn’t used often, the less overall impact it has.\nSo the bottleneck of the program becomes the part that we didn\u0026rsquo;t optimize.\nThe author correlates the law to parallel computing. No matter how many cores we have, if some parts of the computation cannot be parallelized will be the bottleneck. Computers at low utilization use little power\nIn 2012, 33% of peak power was used at 10% of the load. This is important because most of the time, server workloads are between 10-50% (in Google\u0026rsquo;s warehouse scale computers) and 100% at 1% of the time.\nMy opinion at this point in time (where I haven\u0026rsquo;t read the following chapters) would be that the reason we use so much power is due to leakage current. In the power wall section, it was mentioned that 40% of power consumption in chips today are due to leakage.\nOptimizing for speed has no connection to optimizing for energy efficiency\nHere, the explanation is simple, since energy is power over time, the less time you spend executing a program, the less energy you spend.\nJudging performance based on only part of the full performance equation, instead of considering all contributing factors\nIt was emphasized a lot during performance section, that we should use clock rate, instruction count and CPI to evaluate. Skipping one or two factors is a common pitfall.\nAuthors brought in another alternative (performance metric) to time - MIPS or Millions of Instructions Per Second. This should not be confused with MIPS (Microprocessor without Interlocked Pipeline Stages).\nMIPS is program execution rate, meaning higher the number, the faster the computer. But there are 3 problems with MIPS:\nSince MIPS only cares about the numbers and does not take account of CPI, it cannot be used to compare computers with different instruction sets. MIPS varies from program to program, so we must use multiple MIPS. Figure 1.18 from the book: Program execution time varies independently of MIPS. If one program has more instructions but each instruction is faster, MIPS is different but independent of actual performance. MIPS = Instruction count / (Execution time * 10^6) References:\nHennessy, J. L., \u0026amp; Patterson, D. A. (2018). Computer Organization and Design: RISC-V Edition (2nd ed.). Morgan Kaufmann/Elsevier.\n","externalUrl":null,"permalink":"/posts/computer-organization/patterson--hennessy-2020/chapter-1/part-3/","section":"Posts","summary":"Performance # When you are choosing a computer, how should you choose?","title":"Part 3: Performance, Power \u0026 the “Sea Change”","type":"posts"},{"content":" Contributing \u0026amp; Versioning # This document outlines how to contribute to RVSvKit, our branching and release policies, and our roadmap for future enhancements. It’s designed to align with the Overview and Module Inventory docs.\n1. Contributing Guidelines # Contributions Status\nAt present, RVSvKit is not accepting external PRs until we formalize the module API and CI stability.\nWe plan to open contributions when the core API (under modules/, common/, protocols/) reaches version 1.0.0.\nFuture Process\nWe will provide a CONTRIBUTING.md template with:\nIssue template (bug report, feature request)\nPR template (description, test instructions, impact analysis)\nAll contributions will require:\nPass all CI pipelines (verilator.yml, icarus.yml, vivado_docker.yml)\nAdhere to 03-coding-conventions.md style rules\nUpdate ci/benchmark CSV if resource usage changes\nInclude or update tests in the appropriate tb/ directory\n2. Branching \u0026amp; Release Policy # Branching Model\nmain branch holds the latest stable release.\nFeature branches prefixed with feature/ for new API or core modules.\nHotfix branches prefixed with hotfix/ for critical bug fixes against releases.\nVersioning\nWe follow Semantic Versioning: MAJOR.MINOR.PATCH\nMAJOR: incompatible API or backward-breaking changes\nMINOR: backward-compatible feature additions\nPATCH: backward-compatible bug fixes\nRelease Checklist\nAll CI pipelines pass\nChangelog entry in docs/CHANGELOG.md\nTag release in Git: vX.Y.Z\nUpdate version in common/pkg/common_pkg.sv header comment\nPublish GitHub release with release notes\n3. Roadmap \u0026amp; Future Improvements # We strive for incremental progress with clear ETAs (quarters refer to calendar year 2025):\nFeature Status ETA CSR file \u0026amp; trap handler In design Q3 2025 Multiplier/divider variants Planned Q4 2025 Cache enhancements (write-back) Planned Q1 2026 MMU stubs \u0026amp; TLB interfaces Research Q2 2026 DRAM controller (DDR3/DDR4) Future Q3 2026 Vector unit (RVV lanes) Future Q4 2026 4. Open Ideas \u0026amp; Feedback # We welcome internal team discussions via GitHub Discussions or project boards before opening to the public.\nIf you have suggestions—module ideas, protocol enhancements, automation tips—please document them as GitHub Issues under the appropriate label: enhancement, performance, or docs.\nRefer to 01-overview for full repo layout, 02-module-inventory for module catalog, and 03-coding-conventions for pipeline details.\n","externalUrl":null,"permalink":"/posts/rvsvkit/07-contributing-and-versioning/","section":"Posts","summary":"Contributing \u0026amp; Versioning # This document outlines how to contribute to RVSvKit, our branching and release policies, and our roadmap for future enhancements.","title":"07-contributing-and-versioning","type":"posts"},{"content":" Benchmarking the Intel Core i7 # SPEC CPU Benchmark # If you had 2 computers, how would you know one of them performed better than the other? Whichever computer that completes the task faster is the better computer. But to make it a fair competition, you would have to see which one performs better on the same workload.\nThe various programs used to test the performance is called benchmarks. As mentioned in \u0026ldquo;The 8 Great Ideas in Computer Architecture\u0026rdquo;, to make the common case fast, we need to know what the common case is.\nOne benchmark introduced by computer vendors is System Performance Evaluation Cooperative (SPEC). It has 12 integer benchmarks and 17 floating point benchmarks.\nThe integer benchmarks go from C compiler to chess engine to Quantum simulation. The floating point benchmarks include structured grid codes for finite element modeling, particle method codes, to sparce linear algebra codes.\nTo make it easier for making comparisons, we use SPECratio. $$\rSPEC Ratio = \\frac{\\text{Measured Time}}{ \\text{Reference Time}}​\r$$ This gives individual performance metric for each of the programs in SPEC benchmark. We can then take the geometric mean of the ratios. $$\rGeoMean=(∏_{i=1}^n​SPEC Ratio_i​)^{1/n}\r$$ This geometric mean is used because it produces the same result regardless of the computer that was used as the reference.\nSPEC Power Benchmark # The power benchmark was added later after it became apparent how important power consumption was.\nIn the **SPECpower benchmark, the energy efficiency of a server is measured by the number of server-side Java operations performed per watt of power consumed.\nThe system is tested at 11 load levels (from 0% to 100% in 10% increments). The ssj_ops/watt score is computed by summing the performance across all load levels and dividing by the total average power:\n$$\r\\text{ssj\\_ops/watt} = \\frac{\r\\sum_{i=0}^{10} \\text{ssj\\_ops}\\_i\r}{\r\\sum_{i=0}^{10} \\text{power}\\_i\r}\r$$ Where:\n$\\text{ssj\\_ops}\\_i$: Average server-side Java operations per second at load level $i$ $\\text{power}\\_i$: Average power in watts at load level $i$ $i = 0, 1, 2, ..., 10$: Corresponding to 0% through 100% load levels It measures processors, caches, main memory as well as Java virtual machine, compiler, garbage collector, and pieces of operating system.\nConcluding Remarks # This part concluded the chapter again bringing attention to the 8 great ideas, evaluating the performance of computers, current constraints and elaborated on the 5 classic components of the computer and how they serve as a framework for the rest of the book.\nChapter 3: Datapath\nChapter 4: Datapath, Control (what we also call processor)\nChapter 5: Memory, Input, Output\nChapter 6: Datapath, Control, Input, Output\nAppendix B: Datapath, Control\nI would like to formally end the summary for Chapter 1 with a quote mentioned in the book by Alfred North Whitehead in his 1911 book \u0026ldquo;An Introduction to Mathematics.\u0026rdquo;\n“Civilization advances by extending the number of important operations which we can perform without thinking about them”\nReferences: Hennessy, J. L., \u0026amp; Patterson, D. A. (2018). Computer Organization and Design: RISC-V Edition (2nd ed.). Morgan Kaufmann/Elsevier.\n","externalUrl":null,"permalink":"/posts/computer-organization/patterson--hennessy-2020/chapter-1/part-4/","section":"Posts","summary":"Benchmarking the Intel Core i7 # SPEC CPU Benchmark # If you had 2 computers, how would you know one of them performed better than the other?","title":"Part 4: Real-World Examples \u0026 Wrap-Up","type":"posts"},{"content":" Automation \u0026amp; Optimization # This document describes the scaffolding tools, CI/CD automation, parameter sweeps, RTL optimization guidelines, and static-analysis reporting for RVSvKit, consistent with Overview and Module Inventory.\n1. Code Scaffolding (in scripts/) # scaffold_module.py\nGenerates new module directory under modules/\u0026lt;category\u0026gt;/\u0026lt;name\u0026gt;/ with src/, tb/, and README stub.\nscaffold_pkg.py\nCreates package skeleton in common/pkg/ with standard header comment and CHANGELOG.md entry.\n2. CI/CD Automation # GitHub Actions\nFunctional CI (verilator.yml, icarus.yml) on PR/push.\nSynthesis CI (vivado_docker.yml) nightly.\nScheduled Tasks\nWeekly static timing analysis on examples/simple_soc.\nDaily security scan of Docker images for known CVEs.\n3. Parameter Sweeps \u0026amp; Auto‑Tuning # sweep_params.py\nAutomates multi-dimensional sweeps over DATA_WIDTH, UNICORE flag, pipeline depth.\nCollects area, timing, and power metrics into ci/benchmarks/ CSV files.\nAuto‑Tuner\nFuture integration: use scripts to search for Pareto‑optimal configurations and update thresholds.json automatically.\n4. RTL Optimization Guidelines # Multi‑Cycle Operator Pipelining\nAutomate insertion of pipeline registers around long operators (e.g. multipliers) via scripts. Resource Utilization Hooks\nEmbed synthesis pragmas (/* synthesis syn_use_dsp=1 */) parameterized by USE_DSP flag. Consistency with Style\nGenerated code honors 03-coding-conventions.md (indentation, naming, non‑blocking assignments). 5. Static Analysis \u0026amp; Reporting # Linting\nsvlint scans for anti-patterns, mixed blocking/non‑blocking usage. Assertion Coverage\nReport assertion pass/fail counts per module. Security Checks\nEnforce no unauthorized DPI imports; flag use of import \u0026quot;DPI-C\u0026quot;. Dashboard Generation\nUse a script to convert CSV benchmarks into HTML dashboard under docs/benchmarks/. ","externalUrl":null,"permalink":"/posts/rvsvkit/06-automation-optimization/","section":"Posts","summary":"Automation \u0026amp; Optimization # This document describes the scaffolding tools, CI/CD automation, parameter sweeps, RTL optimization guidelines, and static-analysis reporting for RVSvKit, consistent with Overview and Module Inventory.","title":"06-automation-optimization","type":"posts"},{"content":" CI \u0026amp; Testing # This document defines the continuous-integration pipelines, smoke synthesis, functional regression, benchmarking, and gating thresholds for RVSvKit, consistent with the Overview and Module Inventory.\n1. CI Pipelines (in ci/) # Pipeline Tool Trigger Purpose verilator.yml Verilator PR / push to main Functional lint \u0026amp; simulation icarus.yml Icarus Verilog PR / push to main RTL simulation with testbenches vivado_docker.yml Vivado (Docker) Nightly / scheduled Synthesis smoke, area \u0026amp; timing measurements Each pipeline reports pass/fail status, code coverage, and synthesis metrics via badges in README.md.\n2. Functional CI # Regression tests:\nRun all testbenches under modules/**/tb/ using Verilator \u0026amp; Icarus.\nGate on 0 new assertions, no regressions, and ≥ 95% line coverage per module.\nCoverage collection:\nVerilator generates coverage data (.gcov).\nCoverage thresholds defined in ci/thresholds.json.\n3. Synthesis CI # Smoke Synthesis:\nQuick synth of each module wrapper (in ci/smoke/) at default parameters.\nCapture utilization: LUTs, FFs, BRAMs, DSPs.\nBaseline thresholds:\nDefined in ci/thresholds.json (e.g., adder: LUTs ≤ 50, multiplier: LUTs ≤ 200).\nFail CI if any metric exceeds its threshold.\nTiming checks:\nReport worst negative slack (WNS) at typical PVT corner.\nGate on WNS ≥ −1 ns margin.\n4. Benchmarking \u0026amp; Metrics # Historical logging:\nCSV files under ci/benchmarks/ record date, module name, LUTs, FFs, WNS, coverage.\nPRs that change resource usage must include updated CSV entries.\nTrend visualization:\nScripts in scripts/sweep_params.py can plot resource trends over time. 5. Additional Quality Gates # Linting:\nEnforce SV style via svlint matching 03-coding-conventions.md. Static Timing Analysis (STA):\nFull P\u0026amp;R run on examples/simple_soc scheduled weekly. Security \u0026amp; Dependency Checks:\nScan for unapproved DPI imports.\nCheck Docker images for CVEs.\n","externalUrl":null,"permalink":"/posts/rvsvkit/05-ci-and-testing/","section":"Posts","summary":"CI \u0026amp; Testing # This document defines the continuous-integration pipelines, smoke synthesis, functional regression, benchmarking, and gating thresholds for RVSvKit, consistent with the Overview and Module Inventory.","title":"05-ci-and-testing","type":"posts"},{"content":"","externalUrl":null,"permalink":"/posts/computer-organization/patterson--hennessy-2020/chapter-5/","section":"Posts","summary":"","title":"Chapter 5","type":"posts"},{"content":" Packages \u0026amp; Interfaces # This document outlines the shared packages and bus-interface abstractions for RVSvKit, consistent with the Overview and Module Inventory.\n1. Key Considerations # Separation of Concerns\ncommon/pkg/ holds pure types, parameters, macros.\nprotocols/\u0026lt;bus\u0026gt;/interface/ defines signal bundles and handshakes.\nprotocols/\u0026lt;bus\u0026gt;/adapters/ bridges two protocol domains—no mixed responsibilities.\nParametrization \u0026amp; Scalability\nExpose ADDR_WIDTH, DATA_WIDTH, XLEN, ID_WIDTH in packages and interfaces for easy scaling.\nInterfaces use generics (parameters) rather than hard-coded bus widths.\nNaming \u0026amp; Organization\nPackage files: \u0026lt;name\u0026gt;_pkg.sv under common/pkg/.\nInterface files: \u0026lt;bus\u0026gt;_if.sv under protocols/\u0026lt;bus\u0026gt;/interface/.\nAdapter files: \u0026lt;from\u0026gt;_to_\u0026lt;to\u0026gt;.sv under protocols/\u0026lt;bus\u0026gt;/adapters/.\nHandshake Semantics\nClearly document valid-ready or ready-valid timing relationships in each interface.\nUse a shared handshake_if or utility tasks from packages to enforce back-pressure safely.\nData \u0026amp; Control Field Mapping\nBefore coding adapters, tabulate how fields map:\nTileLink.opcode → AXI4.awprot/arprot TileLink.size → AXI4.awlen/arlen conversion Testing Strategy\nBehavioral testbenches for every interface and adapter in protocols/\u0026lt;bus\u0026gt;/tb/.\nUse common verification tasks from common/pkg/ to reduce duplication.\nDocumentation \u0026amp; Diagrams\nKeep timing diagrams and mapping tables in docs/protocols/.\nReference spec sections (RISC‑V Privileged Spec, TileLink spec) alongside your prose.\nVersion Control \u0026amp; Change Tracking\nAnnotate package versions in header comments (e.g. // Version 0.1.0).\nMaintain a CHANGELOG.md under common/pkg/ for interface evolution.\n2. Directory Layout # RVSvKit/ ├── common/ │ ├── pkg/ │ │ ├── common_pkg.sv # types, parameters, macros │ │ ├── interfaces_pkg.sv # shared interface helpers │ │ └── macros_pkg.sv # utility macros │ └── utils/ │ └── assertions.sv # common assertion tasks ├── protocols/ │ ├── tilelink/ │ │ ├── interface/ │ │ │ └── tilelink_if.sv # TileLink bundle (A/B/C/D/E channels) │ │ ├── adapters/ │ │ │ ├── tl_to_axi4lite.sv │ │ │ └── axi4lite_to_tl.sv │ │ └── tb/ │ └── axi4lite/ │ ├── interface/ │ │ └── axi4lite_if.sv │ ├── adapters/ │ └── tb/ └── docs/ └── protocols/ ├── tilelink.md # timing diagrams \u0026amp; field mappings └── axi4lite.md # protocol overview \u0026amp; semantics 3. Interface Definition Guidelines # Header Comment\n//------------------------------------------------------------------------- // Interface : tilelink_if // Version : 0.1.0 // Author : Connie // Date : 2025-06-17 // Brief : Bundle for TileLink A/B/C/D/E channels with valid-ready. //------------------------------------------------------------------------- Interface Block\nParameterize widths:\ninterface tilelink_if #( parameter int XLEN = 32, parameter int TL_DATA_WIDTH = 64 ) (input logic clk, input logic rst_n); logic valid; logic ready; logic [7:0] opcode; logic [TL_DATA_WIDTH-1:0] data; // … other fields … endinterface Adapter Module\nImport relevant packages:\nimport common_pkg::*; import interfaces_pkg::*; Preserve valid-ready semantics between interfaces.\n4. Adapter Field Mapping Table # Source Field Dest Field Conversion Notes TL.opcode AXI.awprot/arprot Pack opcode bits into prot fields TL.size AXI.awlen/arlen Scale from bytes to beat counts TL.source AXI.arid Direct map TL.mask AXI.wstrb Inverse mapping of byte-mask bits 5. Testing Templates # module tilelink_if_tb; // Instantiate interface and simple driver tilelink_if #(.TL_DATA_WIDTH(64)) tl_if(clk, rst_n); initial begin // Cycle through valid-ready handshake tl_if.valid \u0026lt;= 1; wait (tl_if.ready); // … assert data integrity … end endmodule Use similar skeletons for all adapters in protocols/\u0026lt;bus\u0026gt;/tb/.\n","externalUrl":null,"permalink":"/posts/rvsvkit/04-packages-and-interfaces/","section":"Posts","summary":"Packages \u0026amp; Interfaces # This document outlines the shared packages and bus-interface abstractions for RVSvKit, consistent with the Overview and Module Inventory.","title":"04-packages-and-interfaces","type":"posts"},{"content":"","externalUrl":null,"permalink":"/posts/computer-organization/patterson--hennessy-2020/chapter-4/","section":"Posts","summary":"","title":"Chapter 4","type":"posts"},{"content":" Coding Conventions # This guide defines the SystemVerilog style and naming rules for RVSvKit, ensuring consistency with the Overview and Module Inventory.\n1. File \u0026amp; Module Naming # File names: lowercase_with_underscores.sv (e.g., half_adder.sv, load_store_unit.sv)\nModule \u0026amp; interface names: match file name exactly (no extension), in snake_case (e.g., module half_adder)\nPackage names: suffix _pkg (e.g., common_pkg.sv, interfaces_pkg.sv)\nTestbench files: same basename + _tb.sv (e.g., half_adder_tb.sv)\n2. Directory Structure # Modules live under modules/\u0026lt;category\u0026gt;/\u0026lt;block\u0026gt;/src/ and testbenches under tb/.\nE.g.:\nmodules/arithmetic/half_adder/src/half_adder.sv modules/arithmetic/half_adder/tb/half_adder_tb.sv 3. Coding Style \u0026amp; Formatting # Indentation: 2 spaces (no tabs)\nLine length: wrap at 100 characters\nBlocks: braces on the same line as statement\nPort lists: one port per line, aligned\nmodule half_adder #( parameter int DATA_WIDTH = 1 ) ( input logic [DATA_WIDTH-1:0] a, input logic [DATA_WIDTH-1:0] b, output logic [DATA_WIDTH-1:0] sum, output logic carry ); 4. Procedural Blocks # Combinational: use always_comb (not always @(*))\nSequential: use always_ff @ (posedge clk) with adjacent reset\nNon-blocking vs blocking:\nSequential (always_ff): non-blocking (\u0026lt;=) only\nCombinational (always_comb): blocking (=) only\n5. Case Statements \u0026amp; Enums # Use unique or priority for case statements in FSMs or decoders\nPrefer enum logic [1:0] {S_IDLE, S_BUSY} for state machines\nList all enum values in case and include a default: assert(0);\n6. Parameters \u0026amp; Localparams # Parameter names: UPPER_SNAKE_CASE (e.g., DATA_WIDTH, ADDR_WIDTH)\nLocalparam names: prefix LP_ (e.g., localparam int LP_STAGE_COUNT = 5)\n7. Constants \u0026amp; Signals # Signal names: snake_case (e.g., valid_in, ready_out)\nConstant nets: UPPER_SNAKE_CASE for macros (in macros_pkg) or parameters\n8. Interface \u0026amp; Package Usage # Group related signals in interfaces; suffix names with _if (e.g., tilelink_if)\nImport packages at top of file:\nimport common_pkg::*; import interfaces_pkg::*; 9. Assertions \u0026amp; Coverage # Place assertions in common/utils/assertions.sv\nUse immediate assertions in sequential blocks and concurrent assertions for properties:\nlogic valid_reg; always_ff @(posedge clk) begin assert (!valid_reg || ready) else $error(\u0026#34;Backpressure violation\u0026#34;); end 10. Comments \u0026amp; Documentation # Use // for inline comments and /* … */ for multi-line blocks\nFor public modules/interfaces, include a header comment with:\nBrief description\nAuthor \u0026amp; date\nParameters \u0026amp; ports summary\n","externalUrl":null,"permalink":"/posts/rvsvkit/03-coding-conventions/","section":"Posts","summary":"Coding Conventions # This guide defines the SystemVerilog style and naming rules for RVSvKit, ensuring consistency with the Overview and Module Inventory.","title":"03-coding-conventions","type":"posts"},{"content":"","externalUrl":null,"permalink":"/posts/computer-organization/patterson--hennessy-2020/chapter-3/","section":"Posts","summary":"","title":"Chapter 3","type":"posts"},{"content":" Module Inventory # This document catalogs every leaf RTL block under modules/.\n(For full repo layout, refer to 01-overview.)\nFolder Sub-tree # RVSvKit/ └── modules/ ├── arithmetic/ │ ├── half_adder/ │ ├── adder/ │ ├── multiplier/ │ └── divider/ ├── logic/ │ ├── comparator/ # EQ, NE, SLT, SGE, etc. │ ├── bitwise_and/ │ ├── bitwise_or/ │ └── bitwise_xor/ ├── register/ │ ├── register/ # single-flop with sync reset │ ├── pipeline_reg/ # ready-valid pipeline stage │ └── reg_file/ # parameterized register file ├── control/ │ ├── immediate_generator/ # I/S/B/U/J-type extraction │ ├── branch_unit/ # branch comparison \u0026amp; target logic │ └── control_fsm/ # simple state-machine for pipeline control ├── memory/ │ ├── load_store_unit/ # aligned/misaligned loads \u0026amp; stores │ ├── fifo/ # small synchronous FIFO │ └── cache_wrap/ # write-through cache wrapper └── pipeline/ ├── decoupled_reg/ # ready-valid latch ├── hazard_unit/ # RAW/WAR/WAW detection └── forwarding_unit/ # bypass multiplexers 1. Initial Modules # arithmetic/ # half_adder — 1-bit sum \u0026amp; carry\nadder — N-bit add/sub with carry in/out\nmultiplier — iterative or Booth-based multiply\ndivider — restoring/non-restoring divider\nlogic/ # comparator — EQ, NE, SLT, SGE (signed/unsigned)\nbitwise_and, bitwise_or, bitwise_xor\nregister/ # register — parameterized flop with synchronous reset\npipeline_reg — ready-valid pipeline stage register\nreg_file — multi-port register file\ncontrol/ # immediate_generator — extracts I/S/B/U/J immediates\nbranch_unit — computes branch decisions \u0026amp; targets\ncontrol_fsm — FSM for simple instruction sequencing\nmemory/ # load_store_unit — performs aligned loads/stores, raises misalign trap\nfifo — synchronous FIFO with almost-full/empty flags\ncache_wrap — write-through cache interface wrapper\npipeline/ # decoupled_reg — back-pressure-aware pipeline latch\nhazard_unit — detects RAW/WAR/WAW hazards\nforwarding_unit — data bypass network\n2. Future Modules # CSR file \u0026amp; trap handler — full privileged CSR set, exception delegation\nMultiplier/divider variants — pipelined, SRT, non-restoring algorithms\nCache enhancements — write-back, set-associative, MESI protocol\nMMU stubs — simple page tables \u0026amp; TLB interfaces\nDRAM controller — AXI-compliant DDR3/DDR4 interface\nVector unit — RVV-style SIMD lanes\n3. Protocol Adaptors (in protocols/) # See docs/05-ci-and-testing.md and docs/04-packages-and-interfaces.md for full TileLink ↔ AXI4-Lite, APB adapter listings.\n","externalUrl":null,"permalink":"/posts/rvsvkit/02-module-inventory/","section":"Posts","summary":"Module Inventory # This document catalogs every leaf RTL block under modules/.","title":"02-module-inventory","type":"posts"},{"content":"","externalUrl":null,"permalink":"/posts/computer-organization/patterson--hennessy-2020/chapter-2/","section":"Posts","summary":"","title":"Chapter 2","type":"posts"},{"content":"This is the first chapter of the book. I divided my thoughts and summary into four sections:\nFoundations \u0026amp; The Eight Great Ideas\nCovers the motivation behind studying computer architecture, the shift from uniprocessor to multicore systems, and introduces eight timeless design principles that shape computing systems—such as abstraction, pipelining, parallelism, and memory hierarchy.\nInside the Machine – Abstraction Layers \u0026amp; Technologies\nExplores the different levels below your program and explains the core technologies used to build modern processors and memory systems.\nPerformance, Power \u0026amp; the “Sea Change”\nDiscusses how to evaluate computer performance using CPI, instruction count, and clock rate, introduces Amdahl’s Law, examines power constraints like the Power Wall, and explains why parallelism has become central in modern architecture.\nReal-World Examples \u0026amp; Wrap-Up\nApplies concepts from earlier sections to benchmark analysis (e.g., Intel Core i7), outlines common fallacies and pitfalls, and introduces the five classic components of a computer as a framework for the rest of the book.\nThere is also history and exercise sections in the chapter. I might be updating in the future in my free time.\n","date":"2 June 2025","externalUrl":null,"permalink":"/posts/computer-organization/patterson--hennessy-2020/chapter-1/","section":"Posts","summary":"This is the first chapter of the book. I divided my thoughts and summary into four sections:","title":"Chapter 1 - Computer Abstractions and Technology","type":"posts"},{"content":" Overview # Welcome to RVSvKit, a lightweight, parameterized SystemVerilog library for building RISC-V–style pipelines and peripherals.\nMotivation \u0026amp; Goals # Provide modular, n-bit–parametrized RTL building-blocks (ALU, registers, control FSMs, CSRs, etc.)\nEnable a smooth CI \u0026amp; verification flow (Verilator, Icarus, Dockerized Vivado)\nOffer clear extension points for caches, MMU stubs, DRAM controllers, vector units\nKey Features # Composable Pipeline Primitives\nDecoupled ready-valid registers, hazard detection, forwarding units, simple 5-stage core example.\nCSR \u0026amp; Trap Handler\nImplements base RISC-V privileged CSRs (mstatus, mtvec, mie, mip, mepc, mcause) and exception flow.\nProtocol Adapters\nTileLink ↔ AXI4-Lite, APB adapters; crossbars, arbiters, fragmenters.\nCI \u0026amp; Benchmarking\nPer-module smoke synthesis, functional regression, area \u0026amp; timing thresholds, coverage gating.\nPrerequisites \u0026amp; Getting Started # Clone repo\ngit clone https://github.com/you/RVSvKit.git cd RVSvKit Tools\nVerilator ≥ 4.x, Icarus Verilog\nDocker \u0026amp; Vivado (2023.2+) for synthesis CI\nFirst build \u0026amp; test\nmake ci-verilator make ci-icarus Folder Hierarchy # Below is the canonical top-level layout for the entire repo.\nFor the detailed breakdown of leaf RTL blocks, see the Module Inventory.\nRVSvKit/ ├── common/ # shared packages \u0026amp; utilities │ ├── pkg/ # types, parameters, macros │ └── utils/ # assertions \u0026amp; small helpers ├── modules/ # all synthesizable RTL blocks ├── protocols/ # TileLink, AXI, APB stacks \u0026amp; adapters ├── examples/ # end-to-end demos (simple_soc, tutorials) ├── docs/ # deep dives, diagrams, tutorials ├── ci/ # GitHub Actions / thresholds ├── scripts/ # scaffolding \u0026amp; automation tools ├── Dockerfile \u0026amp; docker/ # dev container setup ├── .gitignore ├── LICENSE ├── README.md └── CONTRIBUTING.md ","externalUrl":null,"permalink":"/posts/rvsvkit/01-overview/","section":"Posts","summary":"Overview # Welcome to RVSvKit, a lightweight, parameterized SystemVerilog library for building RISC-V–style pipelines and peripherals.","title":"01-overview","type":"posts"},{"content":"","date":"20 June 2025","externalUrl":null,"permalink":"/tags/calas/","section":"Tags","summary":"","title":"Calas","type":"tags"},{"content":"","date":"20 June 2025","externalUrl":null,"permalink":"/","section":"Cloudian","summary":"","title":"Cloudian","type":"page"},{"content":"This is where I will update my daily progress and activities.\n","date":"20 June 2025","externalUrl":null,"permalink":"/posts/learning-logs/","section":"Posts","summary":"This is where I will update my daily progress and activities.","title":"Learning Logs","type":"posts"},{"content":"","date":"20 June 2025","externalUrl":null,"permalink":"/tags/learning_log/","section":"Tags","summary":"","title":"Learning_log","type":"tags"},{"content":"","date":"20 June 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" Week 5 Learning Log (June 16–20, 2025) # 1. Objectives # Library Documentation\nFinalize docs 01–07 for RVSvKit Repository \u0026amp; CI Scaffolding\nSet up the RVSvKit GitHub repo\nAdd scaffolding scripts for build/test automation\nDependency Installation\nVerilator, Icarus, Dockerized Vivado SystemVerilog Fundamentals\nPorts\nNets vs. variables\nTasks vs. functions\n2. Daily Activities # 📅 Monday, June 16 # Docs 01 \u0026amp; 02\nCompleted writing the Overview and Module Inventory documents. Installed Verilator\nInstalled via package manager; ran verilator --version to confirm toolchain. SV Fundamentals – Ports\nReviewed input/output/inout declarations, named vs. ordered port lists, and port-type semantics. 📅 Tuesday, June 17 # Docs 03 \u0026amp; 04\nDrafted Coding Conventions and Packages \u0026amp; Interfaces sections. Installed Icarus\nInstalled iverilog; verified v11.0.0 via iverilog --version. SV Fundamentals – Nets vs. Variables\nStudied wire vs. logic/reg, implicit nets, driver resolution and multiple-driver rules. 📅 Wednesday, June 18 # Docs 05 \u0026amp; 06\nWrote CI \u0026amp; Testing and Automation \u0026amp; Optimization chapters. Dockerized Vivado Setup\nCreated Dockerfile for Vivado 2024.2; built container and ran a simple synthesis smoke test. SV Fundamentals – Tasks vs. Functions\nCompared syntax, side-effects, timing control; wrote examples to illustrate use cases. 📅 Thursday, June 19 # Doc 07\nFinalized Contributing \u0026amp; Versioning, outlined future cache/MMU work and internship context. Initialize RVSvKit Repo\nCreated GitHub repo, pushed initial commit with all docs. Add Scaffolding Scripts\nAdded Makefile, GitHub Actions workflow for lint, simulation, and documentation checks. 📅 Friday, June 20 # Review \u0026amp; Proof\nPolished all seven docs for style consistency; updated cross-references. Integrate Dependencies into CI\nExtended Actions workflow to install Verilator, Icarus, and Docker Vivado before tests. Consolidated SV Notes\nCompiled a cheat-sheet covering ports, nets/variables, tasks/functions for quick reference. 3. Key Learnings # Modular Documentation Workflow\nDividing the seven docs across days ensured steady progress and consistent style.\nEarly CI Integration\nScaffolding scripts and tool installs in CI guarantee reproducible builds and tests.\nContainerized FPGA Toolchain\nDockerizing Vivado simplifies environment setup for all team members.\nSV Fundamentals Mastery\nDeepened understanding of port semantics, net/variable distinctions, and procedural constructs.\n","date":"20 June 2025","externalUrl":null,"permalink":"/posts/learning-logs/week-5/","section":"Posts","summary":"Week 5 Learning Log (June 16–20, 2025) # 1. Objectives # Library Documentation","title":"Week 5","type":"posts"},{"content":" Week 4 Learning Log (June 9–13, 2025) # 1. Objectives # Literature Review \u0026amp; Blogging\nComplete Chapter 1 article on Patterson \u0026amp; Hennessy\nRead Chapters 1, 2, and 4 from Harris \u0026amp; Harris (2022)\nWebsite Development\nImplement table styling and internal linking on Hugo site\nRedesign homepage and publish backlog articles\nProfessional Skill Building\nAttend TA training session\nLearn TCL scripting for tool automation\nIP Catalog Planning\nDraft initial plan for a Verilog library of basic modules (RVSvKit MVP) 2. Daily Activities # 📅 Monday, June 9 # Literature \u0026amp; Blog Writing\nFinished drafting the Chapter 1 article on Patterson \u0026amp; Hennessy\nRead key sections from Harris \u0026amp; Harris (2022)\nWebsite Development\nEnhanced Hugo table styling for better readability\nAdded internal links between existing blog posts\n📅 Tuesday, June 10 # Homepage Redesign\nRemade the site’s homepage layout to match new style guidelines\nPublished archived articles to fill content gaps\n📅 Wednesday, June 11 # TA Training Session\nAttended lab assistant training TCL Scripting\nLearned basic TCL commands for automating Vivado/ModelSim tasks RVSvKit Planning\nSketched module inventory for the Verilog IP library: identified half_adder, adder, register, control FSM, and pipeline primitives as first targets 📅 Thursday, June 12 # Health \u0026amp; Rest\nTook sick leave 📅 Friday, June 13 # Health \u0026amp; Rest\nContinued recovery on sick leave 3. Key Learnings # Computer Architecture Foundations\nConsolidated core concepts from Patterson \u0026amp; Hennessy and Harris \u0026amp; Harris, laying groundwork for blog series.\nSite Usability Improvements\nMastered Hugo’s styling and linking features, significantly improving content navigation.\nLab Operations Insight\nGained clarity on TA responsibilities and lab setup through hands-on training.\nAutomation Skills\nAcquired essential TCL scripting skills to streamline FPGA toolchains in future CI workflows.\nIP Catalog Roadmap\nEstablished the MVP module list for RVSvKit, defining clear extension points for pipeline and arithmetic primitives.\nWork–Life Balance\nRecognized the importance of rest and recovery during periods of illness, ensuring sustainable progress.\n","date":"13 June 2025","externalUrl":null,"permalink":"/posts/learning-logs/week-4/","section":"Posts","summary":"Week 4 Learning Log (June 9–13, 2025) # 1. Objectives # Literature Review \u0026amp; Blogging","title":"Week 4","type":"posts"},{"content":" Week 3 Learning Log (June 2–6, 2025) # 1. Objectives # ALU Implementation \u0026amp; Testing\nDesign a simple 4-bit ALU using Vivado IP Integrator arithmetic blocks (alu_bd.v) Verify functionality via testbench and hardware simulation FPGA Design Flow\nMeet with Sanka to review end-to-end FPGA design flow (synthesis, implementation, place-and-route) Reading \u0026amp; Blogging\nRead Patterson \u0026amp; Hennessy, Chapter 1 (“Computer Abstractions and Technology”) Write blog posts summarizing Chapter 1 divided into four thematic sections: Foundations \u0026amp; The Eight Great Ideas Inside the Machine – Abstraction Layers \u0026amp; Technologies Performance, Power \u0026amp; the “Sea Change” Real-World Examples \u0026amp; Wrap-Up Lab Session\nAttend the Edge AIoT and Microelectronics TA session to receive training, lab components and complete assigned lab 2. Daily Activities # 📅 Monday, June 2 # 4-bit ALU via IP Integrator Opened a new Vivado project. Launched IP Integrator: instantiated Xilinx “Arithmetic \u0026amp; Logic” IP (configured for 4-bit width). Wired up inputs: A[3:0], B[3:0] ports op[2:0] to select ADD, SUB, MULTIPLY, DIVIDE Added constant generator blocks to drive OpSel and tested all combinations. Generated Block Design wrapper and created alu_bd.v top-level module. Testbench \u0026amp; Simulation Wrote alu_tb.v to sweep A/B values and check results for each op value. Ran behavioral simulation in XSim; verified: ADD: correct sum and carry-out SUB: correct two’s-complement difference and borrow flag MULTIPLY: correct product DIVIDE: correct quotient and remainder 📅 Tuesday, June 3 # Reading: P\u0026amp;H Chapter 1, Section 1 (“Foundations \u0026amp; The Eight Great Ideas”)\nCovered motivation for studying computer architecture and the shift from uniprocessor to multicore. Identified the eight design principles: Abstraction Pipelining Parallelism Prediction Memory Hierarchy Hierarchical Protection Reliability Energy Efficiency Took detailed notes on how each principle recurs in modern CPU/SoC design. Blog Drafting: Section 1\nDrafted “Foundations \u0026amp; The Eight Great Ideas”: Motivation for performance (Moore’s Law slowing) Core principles that transcend specific technologies Examples: how pipelining and parallelism appear in multicore CPUs 📅 Wednesday, June 4 # Meeting with Sanka – FPGA Design Flow Review\nDiscussed steps from RTL → synthesis → implementation → place-and-route → timing closure → bitstream generation Edge AIoT \u0026amp; Microelectronics TA Session\nAttended the TA training lab: Received training on Fundamentals of Microelectronics and Digital Systems Design Received lab components (breadboard, transistors, resistors, wiring) Completed initial lab Reading: P\u0026amp;H Chapter 1, Section 2 (“Inside the Machine – Abstraction Layers \u0026amp; Technologies”)\nExplored layers from high-level code down to transistors: ISA, microarchitecture, logic, devices, circuits Reviewed core technologies: static CMOS, SRAM, DRAM, interconnect fabrics Blog Drafting: Section 2\nDrafted “Inside the Machine – Abstraction Layers \u0026amp; Technologies”: 📅 Thursday, June 5 # Reading: P\u0026amp;H Chapter 1, Section 3 (“Performance, Power \u0026amp; the ‘Sea Change’”) Learned performance metrics: CPI (cycles per instruction), instruction count, clock rate Reviewed Amdahl’s Law and its implications for parallelism Understood power constraints: the Power Wall, Dark Silicon, and energy efficiency trends Blog Drafting: Section 3 Drafted “Performance, Power \u0026amp; the ‘Sea Change’”: How to calculate CPU performance using CPI×IC×ClkPeriod Why single-threaded frequency scaling plateaued, necessitating multicore Introduction to power-performance trade-offs and dynamic voltage/frequency scaling (DVFS) 📅 Friday, June 6 # Reading: P\u0026amp;H Chapter 1, Section 4 (“Real-World Examples \u0026amp; Wrap-Up”) Examined Intel Core i7 benchmark analysis: how the eight ideas appear in a commercial CPU Reviewed common fallacies (e.g., “faster clock always wins”) and pitfalls (e.g., ignoring memory latency) Identified the five classic components of a computer: Processing Unit Memory Unit I/O Unit Network/Interconnect Storage Blog Drafting: Section 4 Drafted “Real-World Examples \u0026amp; Wrap-Up”: Applied principles (pipelining, caching) to Intel Core i7 data Summarized the five components as a roadmap for the rest of the book Documentation \u0026amp; Website Updates Uploaded alu_tb.v, testbench, and waveform captures to Hugo site Wrote Week 3 blog posts for all four sections: Foundations, Abstractions, Performance \u0026amp; Power, Examples \u0026amp; Wrap-Up 3. Key Learnings # 4-bit ALU via IP Integrator Leveraged Xilinx arithmetic IP to build ADD, SUB, AND, OR, XOR operations in one block Verified that IP Integrator correctly generated ports and constraints; saw how the block maps to LUTs/FFs FPGA Design Flow Understood the complete Vivado flow from RTL to bitstream, including critical constraint and timing steps P\u0026amp;H Chapter 1 Highlights Foundations \u0026amp; Eight Great Ideas: Core principles (abstraction, pipelining, parallelism, memory hierarchy, etc.) form the basis of all architectures Abstraction Layers: How high-level software ultimately relies on transistor-level implementations; importance of mapping optimizations across layers Performance \u0026amp; Power: Metrics (CPI, instruction count, clock rate), Amdahl’s Law, and power-limited scaling leading to multicore designs Real-World Examples: Intel Core i7 data shows lessons in pipelining, caching, and parallel thread execution; five classic components framework guides subsequent chapters ","date":"6 June 2025","externalUrl":null,"permalink":"/posts/learning-logs/week-3/","section":"Posts","summary":"Week 3 Learning Log (June 2–6, 2025) # 1. Objectives # ALU Implementation \u0026amp; Testing","title":"Week 3","type":"posts"},{"content":"","date":"2 June 2025","externalUrl":null,"permalink":"/tags/computer_architecture/","section":"Tags","summary":"","title":"Computer_architecture","type":"tags"},{"content":"","date":"2 June 2025","externalUrl":null,"permalink":"/tags/risc-v/","section":"Tags","summary":"","title":"Risc-V","type":"tags"},{"content":"","date":"2 June 2025","externalUrl":null,"permalink":"/tags/theory/","section":"Tags","summary":"","title":"Theory","type":"tags"},{"content":"As part of my internship, I decided to read \u0026ldquo;Computer Organization and Design RISC-V edition\u0026rdquo; by David A. Patterson and John L. Hennessy by recommendation from my supervisor Sanka.\nI have written a series of articles as a reference for later to quickly remind myself of the ideas discussed and also, express my thoughts for the book.\n","date":"31 May 2025","externalUrl":null,"permalink":"/posts/computer-organization/patterson--hennessy-2020/","section":"Posts","summary":"As part of my internship, I decided to read \u0026ldquo;Computer Organization and Design RISC-V edition\u0026rdquo; by David A.","title":"Computer Organization and Design RISC-V edition","type":"posts"},{"content":"Here, I will be updating what I learned about computer organization and architecture. The two textbooks I am learning from are Digital Design and Computer Architecture - RISC-V Edition by Sarah L. Harris and David Harris and Computer Organization and Design - The Hardware/Software Interface: RISC-V Edition by David A. Patterson and John L. Hennessy.\n","date":"30 May 2025","externalUrl":null,"permalink":"/posts/computer-organization/","section":"Posts","summary":"Here, I will be updating what I learned about computer organization and architecture.","title":"Computer Organization","type":"posts"},{"content":" Week 2 Learning Log (May 26–30, 2025) # 1. Objectives # HDL Implementation \u0026amp; Simulation Implement and simulate the following combinational primitives: 4-bit Subtractor (subtractor4.v) 2-to-1 Multiplexer (mux2to1.v) 4-to-1 Multiplexer (mux4to1.v) Toolflow Practice Create testbenches to verify each module in XSim Synthesize all Week 2 designs in Vivado and analyze LUT/CLB usage Documentation \u0026amp; Blogging Draft blog posts on subtractor design (two’s-complement) and multiplexer architectures Reading (acquired Friday, May 30) Start reading Harris \u0026amp; Harris and Patterson \u0026amp; Hennessy chapters on combinational components 2. Daily Activities # 📅 Monday, May 26 # 4-bit Subtractor (subtractor4.v) Designed two’s-complement subtractor by inverting B inputs, adding 1 (carry-in) to a 4-bit adder. Wrote subtractor4.v: module subtractor4 ( input [3:0] A, input [3:0] B, output [3:0] D, output BorrowOut ); wire [3:0] B_inv; wire carry_in = 1\u0026#39;b1; assign B_inv = ~B; // bitwise invert B // reuse prop_adder for A + (¬B) + 1 prop_adder adder_inst ( .A (A), .B (B_inv), .CI (carry_in), .SUM (D), .CO (BorrowOut) ); endmodule Created testbench subtractor4_tb.v applying A,B pairs: (4’b0101 – 4’b0011 = 2) (4’b0010 – 4’b0100 = –2) (4’b1000 – 4’b1000 = 0), etc. Simulated in XSim; confirmed correct 4-bit difference and borrow flag. 📅 Tuesday, May 27 # 2-to-1 Multiplexer (mux2to1.v) Wrote mux2to1.v to select between two 8-bit inputs for practice: module mux2to1 #( parameter WIDTH = 8 )( input [WIDTH-1:0] D0, input [WIDTH-1:0] D1, input SEL, output [WIDTH-1:0] Y ); assign Y = SEL ? D1 : D0; endmodule Created mux2to1_tb.v to test all SEL / data combinations (e.g., D0=8’hAA, D1=8’h55). Ran XSim behavioral simulation; verified correct output switching. 📅 Wednesday, May 28 # Meeting with Sanka \u0026amp; Verilog Syntax Review Met with Sanka to go over Verilog syntax nuances: module definitions, always blocks, non-blocking vs. blocking assignments, and best practices for naming conventions. 4-to-1 Multiplexer (mux4to1.v) Extended multiplexer logic to four inputs: module mux4to1 #( parameter WIDTH = 8 )( input [WIDTH-1:0] D0, input [WIDTH-1:0] D1, input [WIDTH-1:0] D2, input [WIDTH-1:0] D3, input [1:0] SEL, output [WIDTH-1:0] Y ); always @(*) begin case (SEL) 2\u0026#39;b00: Y = D0; 2\u0026#39;b01: Y = D1; 2\u0026#39;b10: Y = D2; 2\u0026#39;b11: Y = D3; endcase end endmodule Wrote mux4to1_tb.v to exercise SEL = 00,01,10,11 with distinct patterns on D0–D3. Simulated to confirm correct selection and no glitches. 📅 Thursday, May 29 # Synthesis \u0026amp; Resource Analysis Added subtractor4.v, mux2to1.v, and mux4to1.v to a Vivado project. Ran synthesis for each module: Subtractor4 → used ~5 LUTs (4 for each inverted bit \u0026amp; one for adder instrumentation). Mux2to1 (8-bit) → 8 LUTs (one per bit). Mux4to1 (8-bit) → ~16 LUTs (2:1 trees or equivalent). Reviewed Utilization Reports and CLB Mapping to understand LUT distribution and routing overhead. Blog Writing Drafted a post: “Implementing a 4-bit Two’s-Complement Subtractor” covering: Two’s-complement basics (invert + add 1). Verilog implementation leveraging the existing prop_adder. Simulation results and borrow-out interpretation. Drafted a post: “Multiplexer Architectures in FPGA” covering: 2:1 vs. 4:1 multiplexer logic. LUT-based implementation and resource considerations. Simulation snapshots illustrating glitch-free switching. 📅 Friday, May 30 # Book Access \u0026amp; Reading Received Digital Design \u0026amp; Computer Architecture (Harris \u0026amp; Harris) and Computer Organization \u0026amp; Design (Patterson \u0026amp; Hennessy). Read Harris \u0026amp; Harris, Ch 2 (Sect 2.4 “Adders and Subtractors”) to reinforce subtractor theory and comparator design. Read Harris \u0026amp; Harris, Ch 3 (Sect 3.2 “Multiplexers and Demultiplexers”) for mux implementation details. Read Patterson \u0026amp; Hennessy, Ch 3 (Sect 3.3 “Subtracters and Extensions”) and Ch 2 (Sect 2.2 “R-Type ALU Operations”) for context on how subtractors map to ALU control signals. Made notes on best practices for coding subtractors/multiplexers in Verilog and FPGA-friendly optimizations. 3. Key Learnings # Two’s-Complement Subtraction\nImplemented as A + (~B) + 1; borrow-out corresponds to final carry-out. Reusing a ripple-carry adder greatly simplifies subtractor design. Multiplexer Implementation\n2-to-1 Mux: single LUT per bit when width = 1; for WIDTH \u0026gt; 1, replicate per bit. 4-to-1 Mux: often built as two cascaded 2:1 stages → higher LUT count; careful case coding avoids glitches. Verilog Syntax Refinement\nDistinction between blocking (=) and non-blocking (\u0026lt;=) assignments in sequential logic. Best practices: use clear module port lists, consistent indentation, and meaningful signal names. Importance of always @(*) for purely combinational case statements. Resource Utilization\nSubtractor4 consumed ~5 LUTs + routing. Mux2to1 (8-bit) used 8 LUTs; Mux4to1 (8-bit) used ~16 LUTs. Vivado’s utilization reports help anticipate resource requirements for larger datapaths. Reading Insights\nHarris \u0026amp; Harris Ch 2–3 emphasize building subtractors via inverter + adder and show LUT-based mux implementations. Patterson \u0026amp; Hennessy clarify how ALU control signals select between operations (ADD vs. SUB, etc.), reinforcing Week 3 FSM control concepts. ","date":"30 May 2025","externalUrl":null,"permalink":"/posts/learning-logs/week-2/","section":"Posts","summary":"Week 2 Learning Log (May 26–30, 2025) # 1. Objectives # HDL Implementation \u0026amp; Simulation Implement and simulate the following combinational primitives: 4-bit Subtractor (subtractor4.","title":"Week 2","type":"posts"},{"content":" Week 1 Learning Log (May 19–23, 2025) # 1. Objectives # FPGA Architecture \u0026amp; Tools Understand FPGA internal architecture: CLBs (LUTs, muxes, flip-flops), on-chip SRAM/Block RAM Install and configure Vivado/Vitis 2022.2 on Windows HDL Implementation \u0026amp; Simulation Implement and simulate basic arithmetic primitives: Half Adder (half_adder.v) Full Adder (full_adder.v) 4-bit Ripple Carry Adder (prop_adder.v) Reading \u0026amp; Documentation Read and reviewed Combinational Logic lectures from EE2000 Draft concise write-ups on SRAM cell operation, bistable flip-flops, and LUT fundamentals 2. Daily Activities # 📅 Monday, May 19 # FPGA Architecture Overview Studied CLB internals: each CLB contains LUTs backed by SRAM bits, local multiplexers, and flip-flops Reviewed Block RAM (SRAM-based) structure: how 6-T SRAM cells store truth tables and provide synchronous read/write Explored programmable interconnect fabric: how CLBs interconnect via switch matrices. Planner \u0026amp; Timeline Created a high-level internship timeline, aligning Weeks 1–12 with incremental HDL targets and chapter readings. 📅 Tuesday, May 20 # Vivado/Vitis Installation Downloaded and ran AMD Unified Installer 2022.2 (includes Vivado and Vitis) Configured WebPACK license; confirmed license activation within Vivado. Set up environment variables and verified vivado –version and vitis –version on Windows. 📅 Wednesday, May 21 # Half Adder Implementation Wrote half_adder.v: module half_adder (input a, b, output sum, carry); assign sum = a ^ b; assign carry = a \u0026amp; b; endmodule Created testbench Half_Adder_tb.v; applied all four (a,b) combinations. Ran behavioral simulation in XSim; verified truth-table matches expected sum/carry outputs. Reading Read fundamentals of gates, combinational logic, half-adder/full-adder Read implementation details for sum = a ⊕ b, carry = a ∧ b (). 📅 Thursday, May 22 # Full Adder \u0026amp; 4-bit Ripple Carry Adder Imported half_adder.v into a new Vivado RTL project. Developed full_adder.v by cascading two half-adders plus an OR gate: module full_adder (input a, b, cin, output sum, cout); wire s1, c1, c2; half_adder ha0 (.a(a), .b(b), .sum(s1), .carry(c1)); half_adder ha1 (.a(s1),.b(cin),.sum(sum),.carry(c2)); assign cout = c1 | c2; endmodule Created Full_Adder_tb.v; applied all eight (a,b,cin) vectors in XSim to verify sum/ cout. Extended to 4-bit ripple carry adder (prop_adder.v) by chaining four full_adder instances in Vivado’s IP Integrator (). Simulated prop_adder.v to confirm correct 4-bit addition and carry-propagation behavior. Reading Read about full-adder and ripple-carry adder architectures . Read about cascading full-adders for multi-bit addition. 📅 Friday, May 23 # Synthesis \u0026amp; Resource Analysis Ran synthesis in Vivado for half_adder.v, full_adder.v, and prop_adder.v. Examined Utilization Report: Half Adder → 1 LUT, 0 FFs Full Adder → 2 LUTs + 1 LUT for OR, 3 FFs 4-bit CPA → 4 × (full adder) LUT usage + routing overhead (). Viewed CLB Mapping: traced how LUT outputs feed into adjacent CLBs to propagate carry. Weekly Documentation Wrote three short blog posts: SRAM Basics: detailed 6-T cell operation and how LUT/SRAM bits store truth tables. Bistable Flip-Flops: explained edge-triggered D-FF operation, asynchronous reset, and how FPGA fabric implements them. LUT Internals: described how LUTs map their address bits into SRAM contents to realize arbitrary Boolean functions. 3. Key Learnings # Configurable Logic Block (CLB) A CLB contains several small LUTs, flip-flops, and local multiplexers. The LUT is implemented using SRAM cells to encode up to a 4- or 6-input Boolean function. Look-Up Tables (LUTs) Each LUT uses SRAM bits to store a truth-table; any combination of inputs addresses that table. Post-synthesis, I observed per-LUT utilization metrics. Block RAM (SRAM) Larger on-chip memory blocks consist of arrays of SRAM cells. They are inferred in Verilog via ram_style = “block” or instantiated via IP; useful for data storage in larger designs. Vivado Flow Typical flow: Project setup HDL source \u0026amp; test-bench creation Simulation (XSim behavioral) Synthesis → Implementation (place \u0026amp; route) Bitstream generation → Programming Setting up the correct .xdc constraint file is crucial before implementation. Arithmetic Modules Half Adder: implemented as sum = a ^ b, carry = a \u0026amp; b. Full Adder: two half-adders cascaded + cout = c1 | c2. 4-bit CPA: cascades four full adders; carry-out of stage i feeds carry-in of stage i + 1. Resource scaling: 4-bit CPA uses ~4× LUT resources of a single full adder plus interconnect. 4. Additional Activities # Obsidian → Hugo Integration Initialize an Obsidian vault for note-taking and blog drafts. Configure a Hugo site locally (install Hugo, choose a theme). Link Obsidian’s Markdown folder to Hugo’s content/ directory so notes automatically become Hugo blog posts. Deployment on Vercel Set up a GitHub repository containing the Hugo site. Connect the repo to Vercel for automatic deployments on commits to main. Verify that pushing a new Markdown file (via Obsidian sync) triggers Hugo rebuild and Vercel deployment. ","date":"23 May 2025","externalUrl":null,"permalink":"/posts/learning-logs/week-1/","section":"Posts","summary":"Week 1 Learning Log (May 19–23, 2025) # 1. Objectives # FPGA Architecture \u0026amp; Tools Understand FPGA internal architecture: CLBs (LUTs, muxes, flip-flops), on-chip SRAM/Block RAM Install and configure Vivado/Vitis 2022.","title":"Week 1","type":"posts"},{"content":"Motivation for the internship: # I am passionate about forging a career in research and innovation within computer engineering. This internship offers a unique opportunity to translate my theoretical knowledge into hands-on expertise by designing and building a processor from the ground up.\nThough I’ve enjoyed studying digital logic circuits, I’ve noticed that much of what I learned has faded over time—and I’m determined to refresh and deepen those fundamentals.\nBeyond simply revisiting FPGA workflows, I’m excited to explore computer architecture in depth—an area I haven’t yet had formal coursework in, despite it being central to my degree. By guiding a design through every stage—from HDL simulation to on-board testing—I aim to:\nMaster FPGA toolchains and digital-logic design\nBuild a solid understanding of RISC-V instruction set and datapath organization\nBridge the gap between abstract concepts and real hardware implementation\nCultivate the confidence and skills I need to grow each day as a researcher\nUltimately, I believe this hands-on experience will be the cornerstone of my journey toward becoming the kind of engineer and researcher I aspire to be.\nTimeline # Weeks 1-3 FPGA design # Week 1: Environment \u0026amp; Basics # Setting up Vitis Review Verilog syntax and some combinational circuits Building testbench for simulation Week 2: Core modules # Design and verify an n-bit ALU (add, subtract and logic operations) Implement a simple register file Integrate ALU + simple register file Week 3: Synthesis and Testing on board # Synthesize the design on PYNQ-Z2 Write constraints Load the bitstream and run I/O tests Document timing results and resource utilization Weeks 4-6 Computer architecture # Week 4: Instruction set architecture # Read up on ISA concepts (RISC vs CISC, datapath components) Explore the RISC-V base spec Draw a simplified datapath diagram Week 5: Pipelining \u0026amp; Control # Learn pipeline stages and hazards Simulate a 5-stage pipeline in software Implement hazard detection \u0026amp; forwarding logic on paper Week 6: Memory \u0026amp; I/O # Study memory hierarchy (registers, cache, main memory) Model a cache in simulation (measure hit/miss rates) Review basic I/O interfacing (memory vs port mapped) Weeks 7-9 Implementing RISC V architecture # Week 7: Core Integer Pipeline # Translate the simulated pipeline into HDL modules Implement fetch-decode-execute stages in Verilog Week 8: Completing the Pipeline # Add MEM and WB stages; integrate control signals Test a small instruction sequence on the FPGA Week 9: Extensions and Testing # Implement branches and simple control/status registers Develop a test suite Measure and document performance Weeks 10-12 Improvement and Further Research # Week 10: Optimization # Add hazard reduction features (branch prediction, deeper pipelines) Profile performance improvements Week 11: Advanced Features # Explore floating-point or vector extensions Prototype an exception/interrupt handler Week 12: Documentation and Demo # Prepare a design report Recording a demo of the processor Identify open questions for future work ","date":"20 May 2025","externalUrl":null,"permalink":"/posts/internship-timeline/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eMotivation for the internship: \n    \u003cdiv id=\"motivation-for-the-internship\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#motivation-for-the-internship\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eI am passionate about forging a career in research and innovation within computer engineering. This internship offers a unique opportunity to translate my theoretical knowledge into hands-on expertise by designing and building a processor from the ground up.\u003c/p\u003e","title":"Internship Timeline","type":"posts"},{"content":"","date":"13 June 2022","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"ABMA\n","externalUrl":null,"permalink":"/posts/computer-organization/access-bus-protocol/","section":"Posts","summary":"ABMA","title":"","type":"posts"},{"content":"MIPS (Microprocessor without Interlocked Pipeline Stages)\na classic RISC (Reduced Instruction Set Computer) architecture simplicity, efficiency, and ease of pipelining. clean, regular instruction formats five-stage pipeline found in embedded devices where intruction set is nearly invisible, so it is hard to find a computer to download and run mips programs\ncore principles:\nrisc fixed length instructions simple load/store model ??? large register file orthogonality uniform inst formats consistent use of register operands ???? pipelining friendly minimal data dependencies -\u0026gt; no complex addressing modes interlocks handled in hardware so isa is clean instruction formats: r type -\u0026gt; opcode (6) rs (5) i type -\u0026gt; opcode (6) rs (5) j type -\u0026gt; opcode (6) address (26)\n5 stage pipeline one instruction per cycle\nIF (instruction fetch)\nread next inst from inst memory (PC) ID (instruction decode)\ndecode opcode read regs rs and rt sign-extend immediate ??? EX (execute)\nalu ops calc branch target MEM (memory access)\nload from or store to memory WB (write back)\nwrite alu or mem results back to register file key components\npc inst mem reg (32 x 32 bits) alu w/ control signals data mem sign-extension and mux (selecting inputs) control unit\nmain control decodes 6 bit opcode to generate signals: RegDst, ALUSrc, MemToReg, RegWrite, MemRead, MemWrite, Branch, Jump alu control further decode funct (r type) + 2 bit ALUOp choose exact ALU operation Handling hazards\nData hazards: when an inst depends on a prior inst result forwarding (bypassing) route alu outputs directly to the inputs of a following instruction pipeline stalls (interlocks) hardware inserts \u0026ldquo;bubbles\u0026rdquo; until data is ready Control hazards: from branches and jumps delayed branch instruction immediately after a branch is always executed Branch prediction more advanced mips implementations structural hazards: resource conflicts mips avoids this by having separate inst and data memories (harvard style) and separate alu and registers ports ","externalUrl":null,"permalink":"/posts/computer-organization/mips/","section":"Posts","summary":"MIPS (Microprocessor without Interlocked Pipeline Stages)\na classic RISC (Reduced Instruction Set Computer) architecture simplicity, efficiency, and ease of pipelining.","title":"","type":"posts"},{"content":" Clone \u0026amp; bootstrap git clone + initialize any submodules Install prerequisites: Verilator ≥4.x, Icarus Verilog, Docker \u0026amp; Vivado 2023.2+ Run first build \u0026amp; test: make ci-verilator and make ci-icarus Scaffold new code scripts/scaffold_module.py → module skeletons under modules/\u0026lt;category\u0026gt;/\u0026lt;name\u0026gt;/ scripts/scaffold_pkg.py → package stubs under common/pkg/ Implement initial RTL blocks: Fill out directories per the Module Inventory (e.g. half_adder, adder, multiplier, …) Enforce coding conventions File/module naming, indentation, always_ff/always_comb, case-statement style, etc. Directory layout: modules/\u0026lt;cat\u0026gt;/\u0026lt;block\u0026gt;/src/..., .../tb/... Define packages \u0026amp; interfaces Populate common/pkg/ with \u0026lt;name\u0026gt;_pkg.sv Create protocols/\u0026lt;bus\u0026gt;/interface/\u0026lt;bus\u0026gt;_if.sv + adapters in protocols/\u0026lt;bus\u0026gt;/adapters/ Follow field-mapping tables and handshake semantics Write testbenches Per-module TBs under modules/**/tb/ Interface/adapters TBs under protocols/**/tb/ Use shared assertions in common/utils/assertions.sv Set up CI pipelines ci/verilator.yml (lint+sim) ci/icarus.yml (TB runs) ci/vivado_docker.yml (nightly smoke synth) Configure thresholds in ci/thresholds.json Add smoke-synth wrappers: Quick-synth benches under ci/smoke/ for each module Benchmark logging \u0026amp; trends Update CSVs in ci/benchmarks/ on resource-usage changes Use scripts/sweep_params.py to automate sweeps and populate those CSVs Automate scheduled tasks Weekly STA on examples/simple_soc Daily Docker CVE scan Future: auto-tuner integration for Pareto sweeps Versioning \u0026amp; releases Follow branch model (main, feature/*, hotfix/*) Bump common_pkg header version, add CHANGELOG entry Release checklist: CI pass, changelog, git tag vX.Y.Z, publish notes Contributing guidelines Draft CONTRIBUTING.md with issue/PR templates Disable external PRs until v1.0.0 Documentation polish Cross-link 01-overview, 02-module-inventory, 03-coding-conventions, etc. Add badges (build, coverage, synth metrics) to README.md Examples \u0026amp; tutorials Flesh out examples/simple_soc demo Write step-by-step guides in docs/ ","externalUrl":null,"permalink":"/posts/rvsvkit/progress/to-do/","section":"Posts","summary":"Clone \u0026amp; bootstrap git clone + initialize any submodules Install prerequisites: Verilator ≥4.","title":"","type":"posts"},{"content":" Definition # 555 timers is the most beloved and used integrated circuits, used mainly for timing and waveform generation. It is used in blinking LEDs, PWM, and oscillator circuits.\nModes # It is defined as a \u0026ldquo;highly stable multivibrator circuit\u0026rdquo; - which means it is basically a circuit that can flip between 2 or more voltage stages in a controlled way. It has 3 modes:\nMonostable In this mode, the circuit/timer acts like a stopwatch where you press and button and it gives you one timed pulse\nAstable In this mode, 555 timer is like a blinking light - flipping on and off continuously.\nBistable The last mode is like a light switch. To toggle on or off, you have to do it manually.\nAnatomy: # [Anatomy of a 555 timer from Circuit Basics]\nVoltage divider: From the image you can see there are three 5k$\\ohm$ resistors, hence the name 5-5-5. These resistors divide the power supply to 2/3 VCC and 1/3 VCC.\nTwo Comparators: They compare the voltages (Threshold voltage at pin 6 and Trigger Voltage at pin 2) and set/reset the output.\nSR Flip-Flop: Memory element that controls the output state. The outputs from the comparator feeds into S and R inputs of the flip-flop.\nDischarge capacitor: When values change from 1 to 0, the capacitor discharges the value.\nAmplifier: The output of the SR flip flop also connect with an amplifier before going to the output pin.\n","externalUrl":null,"permalink":"/posts/digital-logic-circuits/555-timers/","section":"Posts","summary":"Definition # 555 timers is the most beloved and used integrated circuits, used mainly for timing and waveform generation.","title":"555 Timers","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":" At the heart of an SRAM cell lies a bistable flip-flop, which is a circuit capable of holding one of two stable states: logic \u0026lsquo;0\u0026rsquo; or logic \u0026lsquo;1\u0026rsquo;. This flip-flop is typically constructed using two cross-coupled inverters\nHow Cross-Coupled Inverters Form a Flip-Flop # A flip-flop is a circuit with two stable states, capable of storing one bit of information. In SRAM, this is achieved by connecting two inverters in a loop:\nInverter A: Takes input from the output of Inverter B.\nInverter B: Takes input from the output of Inverter A.\nThis cross-coupling creates a feedback loop where the output of each inverter reinforces the input of the other. As a result, the circuit stabilizes in one of two states:\nState 1: Output of Inverter A is \u0026lsquo;1\u0026rsquo;; Output of Inverter B is \u0026lsquo;0\u0026rsquo;.\nState 2: Output of Inverter A is \u0026lsquo;0\u0026rsquo;; Output of Inverter B is \u0026lsquo;1\u0026rsquo;.\nThese two stable states correspond to the stored bit being \u0026lsquo;1\u0026rsquo; or \u0026lsquo;0\u0026rsquo;, respectively.\nHere is a graph to illustrate how it stays stable. (From https://engineering.purdue.edu/~vlsi/courses/ee695kr/s2008/Lecture4.pdf)\nHow Transistors form an inverter # A CMOS (Complementary Metal-Oxide-Semiconductor) inverter consists of two types of transistors:\nPMOS (P-type MOSFET): Conducts when the gate voltage is low.\nNMOS (N-type MOSFET): Conducts when the gate voltage is high.\nThese transistors are connected in a specific configuration:\nPMOS Transistor:\nSource connected to the positive supply voltage (VDD).\nDrain connected to the output node.\nGate connected to the input signal.\nNMOS Transistor:\nSource connected to ground (GND).\nDrain connected to the output node.\nGate connected to the input signal.\nThe gates of both transistors are tied together and serve as the input, while their drains are connected together to form the output.\nOperation:\nInput Low (0V):\nPMOS is ON (conducting), NMOS is OFF (non-conducting).\nOutput is pulled up to VDD (logic high).\nInput High (VDD):\nPMOS is OFF (non-conducting), NMOS is ON (conducting).\nOutput is pulled down to GND (logic low).\nThis configuration ensures that the output is always the logical inverse of the input, hence functioning as an inverter.\nReferences # https://engineering.purdue.edu/~vlsi/courses/ee695kr/s2008/Lecture4.pdf\n","externalUrl":null,"permalink":"/posts/digital-logic-circuits/bistable-flip-flop/","section":"Posts","summary":"At the heart of an SRAM cell lies a bistable flip-flop, which is a circuit capable of holding one of two stable states: logic \u0026lsquo;0\u0026rsquo; or logic \u0026lsquo;1\u0026rsquo;.","title":"Bi-stable Flip Flop","type":"posts"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" The architecture of FPGAs rely heavily on Configuration Logic Blocks (CLBs). When you look at it, it is essentially a matrix of these CLBs, wired together by the device\u0026rsquo;s programmable interconnects.\nIt consists of LUTs, FFs (D-flip flop) and a MUX.\n","externalUrl":null,"permalink":"/posts/digital-logic-circuits/clb/","section":"Posts","summary":"The architecture of FPGAs rely heavily on Configuration Logic Blocks (CLBs).","title":"Configuration Logic Block","type":"posts"},{"content":"Nets Nets\n","externalUrl":null,"permalink":"/posts/system-verilog/data-types/","section":"Posts","summary":"Nets Nets","title":"Data Types","type":"posts"},{"content":"This is where I will be adding entries on theories and concepts I learned along the way and would like to look back and reference.\n","externalUrl":null,"permalink":"/posts/digital-logic-circuits/","section":"Posts","summary":"This is where I will be adding entries on theories and concepts I learned along the way and would like to look back and reference.","title":"Digital Logic Circuits","type":"posts"},{"content":"","externalUrl":null,"permalink":"/tags/digital_logic/","section":"Tags","summary":"","title":"Digital_logic","type":"tags"},{"content":" ports getting and passing parameters fork ... join definition of: non-zero simulation-time constructs ] so you can only choose output\u0026rsquo;s type (whether variable or net)\nwhat do you mean by scalar ports, is that just a another way of saying separate ports that are not continuous?\nmore understandable interface examples\nwhat does resolving multiple drivers mean?\nopen-collector or open-drain circuits\n","externalUrl":null,"permalink":"/posts/system-verilog/confused-topics/","section":"Posts","summary":"ports getting and passing parameters fork ... join definition of: non-zero simulation-time constructs ] so you can only choose output\u0026rsquo;s type (whether variable or net)","title":"draft","type":"posts"},{"content":"dedicated hardware blocks optimized for arithmetic-intensive operations—most notably multiplication and accumulation higher performance and lower power for signal-processing algorithms such as filters, transforms, and dot-products\ndot products are signal-processing algorithm?\nBeyond simple MAC, a single DSP slice can be configured to perform:\nMultiply-only or add-only operations\nMultiply-add (MADD) and three-input addition\nBarrel shifting and wide-bus multiplexing\nMagnitude comparison and pattern detection\nSynchronous up/down counting (using the accumulator as a counter)\nCustom bitwise logic functions\nSIMD arithmetic (dual or quad lanes)\nAll these modes are selected via control signals (OPMODE) at runtime\n","externalUrl":null,"permalink":"/posts/digital-logic-circuits/dsp-slices/","section":"Posts","summary":"dedicated hardware blocks optimized for arithmetic-intensive operations—most notably multiplication and accumulation higher performance and lower power for signal-processing algorithms such as filters, transforms, and dot-products","title":"DSP slices","type":"posts"},{"content":" Definition # Look up table is a memory. Instead of recomputing a circuit or a logic function every time, we compute and store in LUTs. Then depending on the input, FPGA looks up from the table and output that result.\nHow it works # Look up tables operates like a memory.\nLet\u0026rsquo;s say you want you implement the Boolean logic ($A$ AND $B$) OR ($\\bar{A}$ AND $\\bar{B}$). One way we can do this is to make the digital circuit. This will however require 3 gates. And most importantly a lot of space with no flexibility. (Once you implement this, you can\u0026rsquo;t really erase it and reuse for another logic function)\nAnother way to do this, is you get the expression and fill out its truth table. $$F = (A \\space AND \\space B) \\space OR \\space (\\bar{A} \\space AND \\space \\bar{B})$$ A B F 0 0 1 0 1 0 1 0 0 1 1 1 Then you store the outputs in the table (F) and store them in a memory, a component we call LUTs. These bits are stored in sram cells in the LUT.\nPhysical implementation # LUTs are built out of SRAM bits to hold the configuration memory (CRAM) LUT-mask and a set of multiplexers to select the bit from CRAM that is going to drive the ouput. A k-input LUTs holds $2^k$ configuration bits. For example, a 4-input LUT can realize any Boolean function of four variables by treating its inputs as the address lines of a 16-entry table whose contents you program. To implement a k-input LUT (k-LUT) - we need $2^k$ SRAM bits and a $2^k$ : 1 multiplexer.\nRetrieving results # During runtime, when you perform the \u0026ldquo;Look up\u0026rdquo; function, your variables/inputs work as the address of the entry you want to look up. We use multiplexers for this where the inputs now are your select lines for the multiplexers.\n","externalUrl":null,"permalink":"/posts/digital-logic-circuits/luts/","section":"Posts","summary":"Definition # Look up table is a memory. Instead of recomputing a circuit or a logic function every time, we compute and store in LUTs.","title":"LUT","type":"posts"},{"content":"Nets model physical connections between drivers. When you have multiple drivers\nAll net types are inherently four-state - each bit can be 0, 1, X (unknown) or Z (high-impedance). This is due to several reasons.\nNets have to have more than 2 states (binary values) because they allow multiple drivers. They must allow X and Z states to model un-driven or partially-driven connections. When there are multiple sources (like bus signals or tri-state buffers), there might be a conflict. When there is a conflict the net will have X value and Z if there is no drivers.\nFloating (Z) # The z state specifically refers to a floating or un-driven condition—the net is not being actively driven by any source.\nWhen there are multiple drivers:\nIf one driver puts z and another puts 1, the result is 1. If one driver puts z and another puts z, the result is z. If one driver puts 1, another puts 0, result is x (conflict). Contention (X) # Since multiple drivers can \u0026ldquo;pushes\u0026rdquo; a value to the same port, there would be some conflict. In that case, simulation resolves it to X to flag the conflict.\nmodule A(output wire a); assign a = 1; endmodule module B(output wire b); assign b = 0; endmodule wire foo; A u1(.a(foo)); B u2(.b(foo)); // foo sees 1 from A and 0 from B → foo becomes X (conflict) Contention Resolution # Whenever there is a conflict, simulation uses a table shown below to resolve the conflict:\nDriver A \\ Driver B 0 1 X Z 0 0 X X 0 1 X 1 X 1 X X X X X ## Net types # Basic Nets Wires require one driver or resolves them using wired logic. It simply represents a physical connection. wire (4-state): the most common net wire [N-1:0] bus is simply an N-bit vector of wires. It is N individual 1-bit nets all bundled under one name. It tells the compiler “I want N separate wires, numbered 0 through N-1\u0026quot;. On an FPGA or ASIC, each bit of that vector becomes its own metal interconnect (or FPGA routing wire). When you hook a 32-bit bus between two modules, the placer \u0026amp; router will physically lay out 32 parallel tracks. Pulled Nets: It is an alias for wire, but used when we expect there to be multiple drivers. It has pull up, which means a default state when there are no drivers. tri0/tri1: when undriven, defaults to 0 or 1. trireg: - like wire, but with an implicit pull-down resistor. Wired-AND / Wired-OR They are resolved net types. Instead of returning x on multiple conflicting drivers, wand and wor resolve the multiple drivers using logic rules: wand: Open-drain: any driver pulling low causes low output. All must be high (or unconnected) for output to be high. wor: Open-source: any driver pulling high causes high output. All must be low (or unconnected) for output to be low. Supply Nets supply1 models a net tied directly to Vcc (logic high). supply0 models a net tied directly to GND (logic low). Summary table # Net Type Value Type 4-State? Multi-Driver? Default Value Resolves? Strength Typical Use Case wire 4-state ✅ Yes 'z (unconnected) ❌ (x on conflict) Medium (strong) General-purpose signal connection tri 4-state ✅ Yes 'z ❌ (x on conflict) Medium (strong) Tri-state buses, shared data lines tri0 4-state ✅ Yes 0 when undriven ❌ Weak pull-down Open-collector lines, bus defaults to 0 tri1 4-state ✅ Yes 1 when undriven ❌ Weak pull-up Open-drain lines, bus defaults to 1 wand 4-state ✅ Yes 'z ✅ (AND) Resolved Wired-AND logic, bus arbitration wor 4-state ✅ Yes 'z ✅ (OR) Resolved Wired-OR logic, interrupt aggregation uwire 2-state ❌ No 0 ❌ Strong Untyped wire; more efficient (for synthesis only) supply0 constant ❌ N/A 0 Constant Strongest Connect to GND, power modeling supply1 constant ❌ N/A 1 Constant Strongest Connect to Vcc, power modeling wand0 4-state ✅ Yes 0 ✅ (AND) Resolved (Rarely used) wired-AND with default 0 wor1 4-state ✅ Yes 1 ✅ (OR) Resolved (Rarely used) wired-OR with default 1 ","externalUrl":null,"permalink":"/posts/system-verilog/nets/","section":"Posts","summary":"Nets model physical connections between drivers. When you have multiple drivers","title":"Nets","type":"posts"},{"content":"Every module or interface exposes a list of ports - points of connection where signal, handshakes or buses flow in and out. They are declared either in Headers or Port-declaration blocks.\nDirections # Direction Role Usage More input Consumed by the module Clock, resets, data inputs, control signals Read-only inside modules output Driven by the module Results, flags and response signals Produces a value, by default is a net (wire), but when they are variable (e.g.logic) type, they behave like registers in procedural blocks inout Bi-directional Shared buses, tri-state pins, wires on I/O Live on a net and allows both the module and the environment to drive value and are typically gated by enable signals Net vs. Variable Ports # Interfaces as Ports # Rather than having\nParameterization # Overriding Parameters # Port-Connection Styles # Reading \u0026amp; Passing Parameters at Runtime # ","externalUrl":null,"permalink":"/posts/system-verilog/ports/","section":"Posts","summary":"Every module or interface exposes a list of ports - points of connection where signal, handshakes or buses flow in and out.","title":"Ports","type":"posts"},{"content":" Language Basics\n1.1. Lexical conventions: identifiers, comments (//, /*…*/), white space\n1.2. Preprocessor: `define, `include, conditionals (ifdef, ifndef)\n1.3. File organization \u0026amp; naming\nData Types \u0026amp; Declarations\n2.1. Nets vs. variables: wire, logic (4-state) vs. bit (2-state)\n2.2. Integer types: byte, shortint, int, longint, integer\n2.3. Real types: real, realtime\n2.4. Enumerations (enum), typedef\n2.5. Composite types: struct, union\n2.6. Arrays: packed vs. unpacked, dynamic arrays, queues, associative arrays\n2.7. Strings\nOperators\n3.1. Arithmetic, relational, logical\n3.2. Bitwise, reduction (\u0026amp;, |, ^, ~\u0026amp;, etc.)\n3.3. Streaming ({\u0026lt;\u0026lt; , \u0026gt;\u0026gt;}) and assignment patterns\n3.4. Inside, wildcard (?), concatenation\nModules \u0026amp; Hierarchy\n4.1. module declaration: ports, parameters, parameters vs. localparams\n4.2. Port lists: ordered vs. named connections\n4.3. Parameterization: default parameters, parameter int WIDTH = …\n4.4. Generate constructs: generate/endgenerate, for–genvar loops\nInterfaces \u0026amp; Packages\n5.1. package basics: sharing typedef, parameters, macros\n5.2. interface blocks: bundling signals, modports\n5.3. Clocking blocks inside interfaces\n5.4. Virtual interfaces for testbench–DUT connection\nProcedural Constructs\n6.1. Always blocks:\n– always_comb for combinational logic\n– always_ff @ (posedge clk…) for sequential\n– always_latch when needed\n6.2. initial and final blocks\n6.3. Control flow: if/else, case (unique/priority), loops (for, while, foreach)\nTasks \u0026amp; Functions\n7.1. function: no timing control, must return a value\n7.2. task: can include delays, no return value\n7.3. System tasks/functions: $display, $finish, $random, etc.\nConcurrency \u0026amp; Timing Control\n8.1. Continuous assignments (assign)\n8.2. Event controls: @ (posedge clk), @*, event triggers\n8.3. Fork–join for parallel processes\nAssertions \u0026amp; Coverage\n9.1. Immediate assertions: assert (…); inside procedural code\n9.2. Concurrent assertions: property, sequence, assert property\n9.3. Coverage: covergroup, coverpoint, cross coverage\nObject-Oriented \u0026amp; Randomization\n10.1. Classes: class definition, constructors, methods\n10.2. Inheritance and virtual methods\n10.3. Randomization: rand/randc, constraint blocks, post_randomize\n10.4. Factory patterns (foundation for UVM)\nDPI \u0026amp; External Interfacing\n11.1. DPI import/export (import \u0026quot;DPI-C\u0026quot; / export \u0026quot;DPI-C\u0026quot;)\n11.2. Interfacing C/C++ routines for modeling or co-simulation\nSynthesis vs. Verification\n12.1. Synthesizable subset: modules, always_ff, always_comb, generate, parameterization\n12.2. Non-synthesizable features: classes, dynamic arrays, randomization, pure simulation constructs\n12.3. Synthesis directives/pragmas (/* synthesis … */)\nPractical Testbench Organization\n13.1. Testbench structure: DUT instantiation, stimulus drivers, monitors\n13.2. Use of program blocks\n13.3. Clock and reset generators (clocking blocks)\n13.4. Integration with simulators \u0026amp; CI (Verilator, Icarus, Vivado)\nCoding Style \u0026amp; Conventions\n14.1. Indentation, naming (snake_case vs. UPPER_SNAKE_CASE)\n14.2. File and module naming (half_adder.sv ⇒ module half_adder)\n14.3. Blocking vs. non-blocking assignments guidelines\n14.4. Header comments and documentation stubs\n","externalUrl":null,"permalink":"/posts/system-verilog/roadmap/","section":"Posts","summary":"Language Basics\n1.1. Lexical conventions: identifiers, comments (//, /*…*/), white space","title":"Roadmap","type":"posts"},{"content":"I am developing my own library / development kit for the ease of use when I experiment more with RISC V architecture. The articles here are documentation and planning for that.\n","externalUrl":null,"permalink":"/posts/rvsvkit/","section":"Posts","summary":"I am developing my own library / development kit for the ease of use when I experiment more with RISC V architecture.","title":"RVSvKit - RISC V System Verilog Kit","type":"posts"},{"content":"","externalUrl":null,"permalink":"/posts/computer-organization/sequential-access-memories/","section":"Posts","summary":"","title":"Sequential access memories","type":"posts"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"SRAMs are static memory. They are implemented using 6 transistors usually and because of that more expensive. Fills 2 needs:\ndirect interface with CPU at speeds not attainable by DRAMs replace DRAMs in systems with very low power consumption In 1st use, SRAM serves as cache memory, interfacing between DRAMs and the CPU For 2nd use, SRAM is used instead of DRAM. This is because DRAM refresh current is several orders of magnitude more than the low-power SRAM standby current.\nAccess time is comparable to DRAMs in low power mode.\nHow it works # Consists of bi-stable flip flops connected to the internal circuity by two access transistors.\nIt has 3 states:\nIdle State When not addressed, the access transistors are off and flip flops maintain their state, preserving the stored data. Read Operation Activating the world line turns on the access transistors, connecting the flip flops to bit lines. Sense amplifiers detect the logic level and transfer it to the output. Write operation Data from the input is driven onto the bit lines, overriding the existing state due to stronger write drivers. Compared to DRAM, SRAM only needs power supply for stable data.\nTypes # 4T Cell Four NMOS transistors with two poly load resistors 6T Cell Four NMOS and two PMOS transistors, (better stability and performance) TFT Cell Utilizes thin-film transistors (often used in applications like display technologies) Applications: # Cache memory: L1 and L2 caches in CPUs Storage buffers: Temporary storage in storage devices Industrial and Peripheral buffers: Networking equipment and other peripherals References: https://web.eecs.umich.edu/~prabal/teaching/eecs373-f11/readings/sram-technology.pdf\n","externalUrl":null,"permalink":"/posts/digital-logic-circuits/sram/","section":"Posts","summary":"SRAMs are static memory. They are implemented using 6 transistors usually and because of that more expensive.","title":"SRAM","type":"posts"},{"content":"System Verilog is a hardware desciption language that builds on Verilog. It adds high-level constructs for design, simulation, and formal verification, making it a popular choice for modern chip design workflows.\n","externalUrl":null,"permalink":"/posts/system-verilog/","section":"Posts","summary":"System Verilog is a hardware desciption language that builds on Verilog.","title":"System Verilog","type":"posts"},{"content":"","externalUrl":null,"permalink":"/tags/system_verilog/","section":"Tags","summary":"","title":"System_verilog","type":"tags"},{"content":"Both functions and tasks are subroutines that are reusable RTL or testbench codes.\nReturn value and Argument directions # Function # must return a single value, whose function you declare in the header arguments are all inputs, cannot declare output or inout ports called from expressions y = my_func(a,b); Task # does not return value directly, to get data out, must use output or inout arguments can have input, output, and inout ports called as a standalone statement my_task(a,b,c,), not inside an expression Timing control and simulation time # Function # cannot include any timing controls (@, wait, #delay) or non-zero simulation-time constructs must execute in zero simulation time - pure combinational logic Task # can include timing controls, delays, event controls, and even fork ... join useful for driving testbenches or modeling multi-cycle behaviors. Usage in RTL vs. testbench # Functions are used in small, combinational operations such as checksum, bit-field packing, and arithmetic helpers. Tasks are used when you need a sequence of events such as stimulus generation, bus transactions, and waiting for handshakes Calling rules and synthesis # Functions # can be called anywhere an expression is allowed (assign statements, inside procedural blocks, and in parameter calculations) synthesizable if they are zero-time and returns single value. Tasks # cannot be used in expressions generally not synthesizable and used most often in testbenches Example # Function # function int add (input a, input int b); add = a + b; endfunction Task # task wait_for_flag( input logic clk, input logic flag, output int count); count = 0; while (!flag) begin @(posedge clk); count++; end endtask ","externalUrl":null,"permalink":"/posts/system-verilog/tasks-vs-functions/","section":"Posts","summary":"Both functions and tasks are subroutines that are reusable RTL or testbench codes.","title":"Tasks vs Functions","type":"posts"},{"content":" Tips to Understand Verilog Code Faster # ✅ 1. Simulate mentally in time steps # Ask yourself:\n\u0026ldquo;At the next rising clock edge, what changes?\u0026rdquo;\nThis helps you walk through how variables evolve over time.\n✅ 2. Track registers vs wires # reg = stores a value across time (needs a clock)\nwire = combines logic (no memory, instant update)\nFocus on what variables remember, and what they compute.\n✅ 3. Highlight State Transitions # If the code uses state variables, mark where it changes.\nMake a state diagram if needed — even for simple ones.\n✅ 4. Look for reset behavior first # Understand what happens during a reset (if (reset)).\nThis tells you the system’s initial condition.\n✅ 5. Recognize common patterns # This one is a pulse generator or oscillator.\nOther common patterns: shift registers, counters, FSMs.\nOnce you’ve seen a few of each, new code becomes very familiar.\n✅ 6. Ignore module ports at first # When you’re just starting out, ignore all input/output declarations and look at the always blocks first — that\u0026rsquo;s where the logic lives.\n","externalUrl":null,"permalink":"/posts/digital-logic-circuits/verilog/","section":"Posts","summary":"Tips to Understand Verilog Code Faster # ✅ 1. Simulate mentally in time steps # Ask yourself:","title":"Understanding verilog","type":"posts"}]